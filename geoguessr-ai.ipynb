{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GeoGuessr Artificial Intelligence Model üåèüåéüåç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import cv2 as cv \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point, Polygon\n",
    "import folium\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# globals\n",
    "dataset_rel_path = \"./data/geotagged_kaggle/streetviews/\"\n",
    "DATASET_ROOT_PATH = os.path.abspath(dataset_rel_path)\n",
    "map_center = [45, 15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading üì∑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The used dataset is based on Google Streetview captures tagged with the latitude and longitude (geoTagged). The images are organized into folders based on the country they are taken from (the name of the folder is the country code).\n",
    "\n",
    "This model works with the countries from Europe (from the [UN's point of view](https://www.worldometers.info/geography/how-many-countries-in-europe/)). Excluded Russia as most of its surface is not in Europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "europe_countries_code_list = [\"AL\", \"AD\", \"AT\", \"BY\", \"BE\", \"BA\", \"BG\", \"HR\", \"CZ\", \"DK\", \"EE\", \"FI\", \"FR\", \n",
    "                              \"DE\", \"GR\", \"VA\", \"HU\", \"IS\", \"IE\", \"IT\", \"LV\", \"LI\", \"LT\", \"LU\", \"MT\", \"MD\", \n",
    "                              \"MC\", \"ME\", \"NL\", \"MK\", \"NO\", \"PL\", \"PT\", \"RO\", \"SM\", \"RS\", \"SK\", \"SI\", \"ES\", \n",
    "                              \"SE\", \"CH\", \"UA\", \"GB\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Insights\n",
    "Let's take a look at the number of images in each folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_files_in_dir(dir_path):\n",
    "    return len(os.listdir(dir_path))\n",
    "\n",
    "def display_number_of_images_in_each_folder():\n",
    "    country_nr_images = {}\n",
    "    for country_folder in os.listdir(DATASET_ROOT_PATH):\n",
    "        if (country_folder in europe_countries_code_list):\n",
    "            country_nr_images[country_folder] = len(os.listdir(os.path.join(DATASET_ROOT_PATH, country_folder)))\n",
    "\n",
    "    plt.bar(country_nr_images.keys(), country_nr_images.values())\n",
    "    plt.xlabel(\"Country\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel(\"Nr of Images\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"European countries that are not in the dataset: \")\n",
    "    print([elem for elem in list(country_nr_images.keys()) + europe_countries_code_list if elem not in country_nr_images.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAG5CAYAAABoRvUVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABGGklEQVR4nO3deXQUVcLG4bc7O4QkBCGBIWyC7JugkEFHwECAsMcFB9nEZdgUULZxAUENi46IIigCCY4IoyAKKMgiMEpYhEGQTVCQqCSoEAJIAiT3+4NDfzRZSHcSOil/zzl1Tqrq3q5bnV7evlV1y2aMMQIAALAou6cbAAAAUJQIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNK8Pd2A4iArK0u//PKLypQpI5vN5unmAACAfDDG6MyZM6pUqZLs9tz7bwg7kn755RdFRER4uhkAAMANSUlJqly5cq7rCTuSypQpI+nykxUUFOTh1gAAgPxIS0tTRESE43s8N4QdyXHoKigoiLADAEAJc71TUDhBGQAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWJq3pxsA4M+p2tiV+S57dHJMEbYEgNXRswMAACyNsAMAACyNsAMAACyNc3YAAJbHOWJ/bvTsAAAASyPsAAAASyPsAAAAS+OcHQAAcsG5PtZAzw4AALA0wg4AALA0wg4AALA0wg4AALA0wg4AALA0wg4AALA0wg4AALA0wg4AALA0wg4AALA0wg4AALA0wg4AALA0wg4AALA0j4edn3/+WQ8++KDKlSungIAANWzYUF9//bVjvTFGzz33nCpWrKiAgABFRUXp0KFDTo9x8uRJ9e7dW0FBQQoJCdHAgQN19uzZG70rAACgGPJo2Dl16pRatWolHx8fffbZZ9q3b59eeeUVlS1b1lFm6tSpmjFjhmbPnq2tW7eqdOnSio6OVnp6uqNM7969tXfvXq1Zs0YrVqzQpk2b9Oijj3pilwAAQDHj7cmNT5kyRREREZo/f75jWfXq1R1/G2M0ffp0PfPMM+rWrZskacGCBQoLC9OyZcvUq1cv7d+/X6tWrdL27dvVvHlzSdLrr7+uTp066eWXX1alSpVu7E4BAIBixaM9O5988omaN2+ue++9VxUqVFDTpk01Z84cx/ojR44oOTlZUVFRjmXBwcFq0aKFEhMTJUmJiYkKCQlxBB1JioqKkt1u19atW3PcbkZGhtLS0pwmAABgTR4NOz/88INmzZqlWrVqafXq1Ro0aJAef/xxJSQkSJKSk5MlSWFhYU71wsLCHOuSk5NVoUIFp/Xe3t4KDQ11lLlWXFycgoODHVNERERh7xoAACgmPBp2srKydOutt+qll15S06ZN9eijj+qRRx7R7Nmzi3S748aN0+nTpx1TUlJSkW4PAAB4jkfDTsWKFVWvXj2nZXXr1tWxY8ckSeHh4ZKklJQUpzIpKSmOdeHh4Tpx4oTT+kuXLunkyZOOMtfy8/NTUFCQ0wQAAKzJo2GnVatWOnjwoNOy7777TlWrVpV0+WTl8PBwrVu3zrE+LS1NW7duVWRkpCQpMjJSqamp2rFjh6PM+vXrlZWVpRYtWtyAvQAAAMWZR6/GGjFihP7617/qpZde0n333adt27bp7bff1ttvvy1JstlsGj58uF544QXVqlVL1atX17PPPqtKlSqpe/fuki73BHXo0MFx+OvixYsaOnSoevXqxZVYAADAs2Hntttu00cffaRx48Zp4sSJql69uqZPn67evXs7yowePVrnzp3To48+qtTUVN1xxx1atWqV/P39HWXee+89DR06VHfffbfsdrtiY2M1Y8YMT+wSAAAoZjwadiSpc+fO6ty5c67rbTabJk6cqIkTJ+ZaJjQ0VAsXLiyK5gEAgBLO42EHQMlWbezKfJc9OjmmCFsCADnz+L2xAAAAihJhBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWJpHw86ECRNks9mcpjp16jjWp6ena8iQISpXrpwCAwMVGxurlJQUp8c4duyYYmJiVKpUKVWoUEGjRo3SpUuXbvSuAACAYsrb0w2oX7++1q5d65j39v7/Jo0YMUIrV67UBx98oODgYA0dOlQ9e/bUV199JUnKzMxUTEyMwsPDtXnzZh0/flx9+/aVj4+PXnrppRu+LwAAoPjxeNjx9vZWeHh4tuWnT5/W3LlztXDhQrVt21aSNH/+fNWtW1dbtmxRy5Yt9fnnn2vfvn1au3atwsLC1KRJE02aNEljxozRhAkT5Ovre6N3BwAAFDMeP2fn0KFDqlSpkmrUqKHevXvr2LFjkqQdO3bo4sWLioqKcpStU6eOqlSposTERElSYmKiGjZsqLCwMEeZ6OhopaWlae/evbluMyMjQ2lpaU4TAACwJo+GnRYtWig+Pl6rVq3SrFmzdOTIEd155506c+aMkpOT5evrq5CQEKc6YWFhSk5OliQlJyc7BZ0r66+sy01cXJyCg4MdU0REROHuGAAAKDY8ehirY8eOjr8bNWqkFi1aqGrVqvrPf/6jgICAItvuuHHjNHLkSMd8WloagQcAAIvy+GGsq4WEhOiWW27R4cOHFR4ergsXLig1NdWpTEpKiuMcn/Dw8GxXZ12Zz+k8oCv8/PwUFBTkNAEAAGsqVmHn7Nmz+v7771WxYkU1a9ZMPj4+WrdunWP9wYMHdezYMUVGRkqSIiMjtWfPHp04ccJRZs2aNQoKClK9evVuePsBAEDx49HDWE899ZS6dOmiqlWr6pdfftH48ePl5eWlBx54QMHBwRo4cKBGjhyp0NBQBQUFadiwYYqMjFTLli0lSe3bt1e9evXUp08fTZ06VcnJyXrmmWc0ZMgQ+fn5eXLXAABAMeHRsPPTTz/pgQce0O+//67y5cvrjjvu0JYtW1S+fHlJ0quvviq73a7Y2FhlZGQoOjpab775pqO+l5eXVqxYoUGDBikyMlKlS5dWv379NHHiRE/tEgAAKGY8GnYWLVqU53p/f3/NnDlTM2fOzLVM1apV9emnnxZ20wAAgEUUq3N2AAAAChthBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWFqBw05aWpqWLVum/fv3F0Z7AAAACpXLYee+++7TG2+8IUk6f/68mjdvrvvuu0+NGjXSkiVLCr2BAAAABeFy2Nm0aZPuvPNOSdJHH30kY4xSU1M1Y8YMvfDCC4XeQAAAgIJwOeycPn1aoaGhkqRVq1YpNjZWpUqVUkxMjA4dOlToDQQAACgIl8NORESEEhMTde7cOa1atUrt27eXJJ06dUr+/v6F3kAAAICC8Ha1wvDhw9W7d28FBgaqSpUqat26taTLh7caNmxY2O0DAAAoEJd7dgYPHqzExETNmzdPX331lez2yw9Ro0aNAp2zM3nyZNlsNg0fPtyxLD09XUOGDFG5cuUUGBio2NhYpaSkONU7duyYYmJiVKpUKVWoUEGjRo3SpUuX3G4HAACwFpd7diSpefPmatSokY4cOaKbb75Z3t7eiomJcbsR27dv11tvvaVGjRo5LR8xYoRWrlypDz74QMHBwRo6dKh69uypr776SpKUmZmpmJgYhYeHa/PmzTp+/Lj69u0rHx8fvfTSS263BwAAWIfLYeePP/7QsGHDlJCQIEn67rvvVKNGDQ0bNkx/+ctfNHbsWJce7+zZs+rdu7fmzJnj1DN0+vRpzZ07VwsXLlTbtm0lSfPnz1fdunW1ZcsWtWzZUp9//rn27duntWvXKiwsTE2aNNGkSZM0ZswYTZgwQb6+vq7uHgA4qTZ2pUvlj052/4cfgKLh8mGscePG6ZtvvtGGDRucTkiOiorS4sWLXW7AkCFDFBMTo6ioKKflO3bs0MWLF52W16lTR1WqVFFiYqIkKTExUQ0bNlRYWJijTHR0tNLS0rR3795ct5mRkaG0tDSnCQAAWJPLPTvLli3T4sWL1bJlS9lsNsfy+vXr6/vvv3fpsRYtWqSdO3dq+/bt2dYlJyfL19dXISEhTsvDwsKUnJzsKHN10Lmy/sq63MTFxen55593qa0AAKBkcrln59dff1WFChWyLT937pxT+LmepKQkPfHEE3rvvfdu+CXr48aN0+nTpx1TUlLSDd0+AAC4cVwOO82bN9fKlf9/DPtKwHnnnXcUGRmZ78fZsWOHTpw4oVtvvVXe3t7y9vbWxo0bNWPGDHl7eyssLEwXLlxQamqqU72UlBSFh4dLksLDw7NdnXVl/kqZnPj5+SkoKMhpAgAA1uTyYayXXnpJHTt21L59+3Tp0iW99tpr2rdvnzZv3qyNGzfm+3Huvvtu7dmzx2nZgAEDVKdOHY0ZM0YRERHy8fHRunXrFBsbK0k6ePCgjh075ghVkZGRevHFF3XixAlHb9OaNWsUFBSkevXqubprAADAglwOO3fccYd27dqlyZMnq2HDhvr888916623Ok4Wzq8yZcqoQYMGTstKly6tcuXKOZYPHDhQI0eOVGhoqIKCgjRs2DBFRkaqZcuWkqT27durXr166tOnj6ZOnark5GQ988wzGjJkiPz8/FzdNQAAYEFujbNz8803a86cOYXdlmxeffVV2e12xcbGKiMjQ9HR0XrzzTcd6728vLRixQoNGjRIkZGRKl26tPr166eJEycWedsAAEDJ4HLYye0ybZvNJj8/vwKNbbNhwwaneX9/f82cOVMzZ87MtU7VqlX16aefur1NAABgbS6HnZCQkDyvuqpcubL69++v8ePHO24lAQAA4Ckuh534+Hg9/fTT6t+/v26//XZJ0rZt25SQkKBnnnlGv/76q15++WX5+fnpn//8Z6E3GAAAwBUuh52EhAS98soruu+++xzLunTpooYNG+qtt97SunXrVKVKFb344ouEHRQJhu8HALjC5eNMmzdvVtOmTbMtb9q0qeM2DnfccYeOHTtW8NYBAAAUkMthJyIiQnPnzs22fO7cuYqIiJAk/f777ypbtmzBWwcAAFBALh/Gevnll3Xvvffqs88+02233SZJ+vrrr3XgwAF9+OGHkqTt27fr/vvvL9yWAgAAuMHlsNO1a1cdPHhQb731lg4ePChJ6tixo5YtW6Zq1apJkgYNGlSojQQAAHCXW4MKVqtWTXFxcYXdFgAAgELnVtiRpD/++EPHjh3ThQsXnJY3atSowI0CAAAoLC6HnV9//VUDBgzQZ599luP6zMzMAjcKAACgsLh8Ndbw4cOVmpqqrVu3KiAgQKtWrVJCQoJq1aqlTz75pCjaCAAA4DaXe3bWr1+vjz/+WM2bN5fdblfVqlXVrl07BQUFKS4uTjExDOAGAACKD5d7ds6dO6cKFSpIksqWLatff/1VktSwYUPt3LmzcFsHAABQQC6Hndq1azsuOW/cuLHeeust/fzzz5o9e7YqVqxY6A0EAAAoCJcPYz3xxBM6fvy4JGn8+PHq0KGD3nvvPfn6+io+Pr6w2wcAAFAgLoedBx980PF3s2bN9OOPP+rAgQOqUqWKbrrppkJtHAAAQEG5Pc7OFaVKldKtt95aGG0BAAAodC6HHWOMPvzwQ33xxRc6ceKEsrKynNYvXbq00BoHAABQUC6HneHDh+utt95SmzZtFBYWJpvNVhTtAgAAKBQuh513331XS5cuVadOnYqiPQAAAIXK5UvPg4ODVaNGjaJoCwAAQKFzOexMmDBBzz//vM6fP18U7QEAAChULh/Guu+++/T++++rQoUKqlatmnx8fJzWM4oyAAAoTlwOO/369dOOHTv04IMPcoIyAAAo9lwOOytXrtTq1at1xx13FEV7AAAACpXL5+xEREQoKCioKNoCAABQ6FwOO6+88opGjx6to0ePFkFzAAAACpdb98b6448/dPPNN6tUqVLZTlA+efJkoTUOAACgoFwOO9OnTy+CZgAAABQNt67GAgAAKCnyHXbS0tLyVY6TlwEAQHGS77ATEhKS55g6xhjZbDZlZmYWSsMAAAAKQ77DzhdffFGU7QAAACgS+Q47d911V1G2AwAAoEi4PM4OAABASULYAQAAlkbYAQAAlpavsLN7925lZWUVdVsAAAAKXb7CTtOmTfXbb79JkmrUqKHff/+9SBsFAABQWPIVdkJCQnTkyBFJ0tGjR+nlAQAAJUa+Lj2PjY3VXXfdpYoVK8pms6l58+by8vLKsewPP/xQqA0EAAAoiHyFnbfffls9e/bU4cOH9fjjj+uRRx5RmTJlirptAAAABZbvQQU7dOggSdqxY4eeeOIJwg4AACgRXL7r+fz58x1///TTT5KkypUrF16LAAAACpHL4+xkZWVp4sSJCg4OVtWqVVW1alWFhIRo0qRJLp+4PGvWLDVq1EhBQUEKCgpSZGSkPvvsM8f69PR0DRkyROXKlVNgYKBiY2OVkpLi9BjHjh1TTEyMSpUqpQoVKmjUqFG6dOmSq7sFAAAsyuWenaefflpz587V5MmT1apVK0nSl19+qQkTJig9PV0vvvhivh+rcuXKmjx5smrVqiVjjBISEtStWzf973//U/369TVixAitXLlSH3zwgYKDgzV06FD17NlTX331lSQpMzNTMTExCg8P1+bNm3X8+HH17dtXPj4+eumll1zdNQAAYEEuh52EhAS988476tq1q2NZo0aN9Je//EWDBw92Kex06dLFaf7FF1/UrFmztGXLFlWuXFlz587VwoUL1bZtW0mXD6HVrVtXW7ZsUcuWLfX5559r3759Wrt2rcLCwtSkSRNNmjRJY8aM0YQJE+Tr6+vq7gEAAItx+TDWyZMnVadOnWzL69Spo5MnT7rdkMzMTC1atEjnzp1TZGSkduzYoYsXLyoqKsppG1WqVFFiYqIkKTExUQ0bNlRYWJijTHR0tNLS0rR3795ct5WRkaG0tDSnCQAAWJPLYadx48Z64403si1/44031LhxY5cbsGfPHgUGBsrPz0//+Mc/9NFHH6levXpKTk6Wr6+vQkJCnMqHhYUpOTlZkpScnOwUdK6sv7IuN3FxcQoODnZMERERLrcbAACUDC4fxpo6dapiYmK0du1aRUZGSrrcw5KUlKRPP/3U5QbUrl1bu3bt0unTp/Xhhx+qX79+2rhxo8uP44px48Zp5MiRjvm0tDQCDwAAFuVyz85dd92l7777Tj169FBqaqpSU1PVs2dPHTx4UHfeeafLDfD19VXNmjXVrFkzxcXFqXHjxnrttdcUHh6uCxcuKDU11al8SkqKwsPDJUnh4eHZrs66Mn+lTE78/PwcV4BdmQAAgDW53LMjSZUqVXLpRGRXZGVlKSMjQ82aNZOPj4/WrVun2NhYSdLBgwd17NgxR49SZGSkXnzxRZ04cUIVKlSQJK1Zs0ZBQUGqV69ekbQPAACULG6FncIybtw4dezYUVWqVNGZM2e0cOFCbdiwQatXr1ZwcLAGDhyokSNHKjQ0VEFBQRo2bJgiIyPVsmVLSVL79u1Vr1499enTR1OnTlVycrKeeeYZDRkyRH5+fp7cNQAAUEx4NOycOHFCffv21fHjxxUcHKxGjRpp9erVateunSTp1Vdfld1uV2xsrDIyMhQdHa0333zTUd/Ly0srVqzQoEGDFBkZqdKlS6tfv36aOHGip3YJAAAUMx4NO3Pnzs1zvb+/v2bOnKmZM2fmWqZq1apunRgNAAD+HFw6QdkYo2PHjik9Pb2o2gMAAFCoXA47NWvWVFJSUlG1BwAAoFC5FHbsdrtq1aql33//vajaAwAAUKhcPmdn8uTJGjVqlGbNmqUGDRoURZsAoNBVG7sy32WPTo4pwpYAuNFcDjt9+/bVH3/8ocaNG8vX11cBAQFO6wtyfywAAIDC5nLYmT59ehE0AwAAoGi4HHb69etXFO0AAAAoEi7fGwsAAKAkyXfPjt1ul81my7OMzWbTpUuXCtwoAACAwpLvsPPRRx/lui4xMVEzZsxQVlZWoTQKAACgsOQ77HTr1i3bsoMHD2rs2LFavny5evfuzT2pAABAsePWvbF++eUXjR8/XgkJCYqOjtauXbsYc6eQuTImiMS4IAAA5MalE5RPnz6tMWPGqGbNmtq7d6/WrVun5cuXE3QAAECxle+enalTp2rKlCkKDw/X+++/n+NhLQAAgOIm32Fn7NixCggIUM2aNZWQkKCEhIQcyy1durTQGgcAAFBQ+Q47ffv2ve6l5wAAAMVNvsNOfHx8ETYDAACgaDCCMgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDS3bhcBa3LlFhXcngIAUFLQswMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNS88BuDTsgMTQAwBKFnp2AACApRF2AACApXEYCwBwQ3HYFDcaPTsAAMDSCDsAAMDSCDsAAMDSOGcHAIA/MVfOoSqp50/RswMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACzNo5eex8XFaenSpTpw4IACAgL017/+VVOmTFHt2rUdZdLT0/Xkk09q0aJFysjIUHR0tN58802FhYU5yhw7dkyDBg3SF198ocDAQPXr109xcXHy9ubKehSOP8OlmQBgVR7t2dm4caOGDBmiLVu2aM2aNbp48aLat2+vc+fOOcqMGDFCy5cv1wcffKCNGzfql19+Uc+ePR3rMzMzFRMTowsXLmjz5s1KSEhQfHy8nnvuOU/sEgAAKGY82vWxatUqp/n4+HhVqFBBO3bs0N/+9jedPn1ac+fO1cKFC9W2bVtJ0vz581W3bl1t2bJFLVu21Oeff659+/Zp7dq1CgsLU5MmTTRp0iSNGTNGEyZMkK+vryd2DQAAFBPF6pyd06dPS5JCQ0MlSTt27NDFixcVFRXlKFOnTh1VqVJFiYmJkqTExEQ1bNjQ6bBWdHS00tLStHfv3hy3k5GRobS0NKcJAABYU7EJO1lZWRo+fLhatWqlBg0aSJKSk5Pl6+urkJAQp7JhYWFKTk52lLk66FxZf2VdTuLi4hQcHOyYIiIiCnlvAABAcVFsws6QIUP07bffatGiRUW+rXHjxun06dOOKSkpqci3CQAAPKNYXK40dOhQrVixQps2bVLlypUdy8PDw3XhwgWlpqY69e6kpKQoPDzcUWbbtm1Oj5eSkuJYlxM/Pz/5+fkV8l4AAIDiyKM9O8YYDR06VB999JHWr1+v6tWrO61v1qyZfHx8tG7dOseygwcP6tixY4qMjJQkRUZGas+ePTpx4oSjzJo1axQUFKR69erdmB0BAADFlkd7doYMGaKFCxfq448/VpkyZRzn2AQHBysgIEDBwcEaOHCgRo4cqdDQUAUFBWnYsGGKjIxUy5YtJUnt27dXvXr11KdPH02dOlXJycl65plnNGTIEHpvAACAZ8POrFmzJEmtW7d2Wj5//nz1799fkvTqq6/KbrcrNjbWaVDBK7y8vLRixQoNGjRIkZGRKl26tPr166eJEyfeqN0Aig1XBj+UGAARwJ+DR8OOMea6Zfz9/TVz5kzNnDkz1zJVq1bVp59+WphNAwAAFlFsrsYCAAAoCsXiaiwAAOA+DmHnjZ4dAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgad6ebgBgZdXGrsx32aOTY4qwJQDw50XPDgAAsDR6dvCn4Uovi0RPCwBYBT07AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0rgRKAAUAW48CxQf9OwAAABLo2cHBebKL1h+vQIAbjR6dgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKV5NOxs2rRJXbp0UaVKlWSz2bRs2TKn9cYYPffcc6pYsaICAgIUFRWlQ4cOOZU5efKkevfuraCgIIWEhGjgwIE6e/bsDdwLAABQnHk07Jw7d06NGzfWzJkzc1w/depUzZgxQ7Nnz9bWrVtVunRpRUdHKz093VGmd+/e2rt3r9asWaMVK1Zo06ZNevTRR2/ULgAAgGLOozcC7dixozp27JjjOmOMpk+frmeeeUbdunWTJC1YsEBhYWFatmyZevXqpf3792vVqlXavn27mjdvLkl6/fXX1alTJ7388suqVKlSjo+dkZGhjIwMx3xaWloh7xkAACguiu1dz48cOaLk5GRFRUU5lgUHB6tFixZKTExUr169lJiYqJCQEEfQkaSoqCjZ7XZt3bpVPXr0yPGx4+Li9Pzzzxf5PgAA/pyqjV2Z77JHJ8cUYUsgFeMTlJOTkyVJYWFhTsvDwsIc65KTk1WhQgWn9d7e3goNDXWUycm4ceN0+vRpx5SUlFTIrQcAAMVFse3ZKUp+fn7y8/PzdDMAAMANUGx7dsLDwyVJKSkpTstTUlIc68LDw3XixAmn9ZcuXdLJkycdZQAAwJ9bsQ071atXV3h4uNatW+dYlpaWpq1btyoyMlKSFBkZqdTUVO3YscNRZv369crKylKLFi1ueJsBAEDx49HDWGfPntXhw4cd80eOHNGuXbsUGhqqKlWqaPjw4XrhhRdUq1YtVa9eXc8++6wqVaqk7t27S5Lq1q2rDh066JFHHtHs2bN18eJFDR06VL169cr1SiwAAPDn4tGw8/XXX6tNmzaO+ZEjR0qS+vXrp/j4eI0ePVrnzp3To48+qtTUVN1xxx1atWqV/P39HXXee+89DR06VHfffbfsdrtiY2M1Y8aMG74vAACgePJo2GndurWMMbmut9lsmjhxoiZOnJhrmdDQUC1cuLAomgcAACyg2J6zAwAAUBgIOwAAwNIIOwAAwNIIOwAAwNL+lCMoAwCAgilJ9/+iZwcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaJygXMVdO4JI8fxIXAMBz+M4oGvTsAAAASyPsAAAASyPsAAAASyPsAAAAS+MEZQAlSkkatRVA8UDPDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDRvTzcAQHbVxq7Md9mjk2OKsCUAUPIRdgDAIgjJQM44jAUAACyNnh0A+JOjRwhWR88OAACwNMIOAACwNA5jAQBKDA65wR2EHXgMH1oAgBuBsGNBhAgAAP4f5+wAAABLo2cHAOAWepFRUtCzAwAALM0yYWfmzJmqVq2a/P391aJFC23bts3TTQIAAMWAJcLO4sWLNXLkSI0fP147d+5U48aNFR0drRMnTni6aQAAwMMscc7Ov/71Lz3yyCMaMGCAJGn27NlauXKl5s2bp7Fjx3q4dQDgGs6FAQpXiQ87Fy5c0I4dOzRu3DjHMrvdrqioKCUmJuZYJyMjQxkZGY7506dPS5LS0tIKvX1ZGX+4VP5KG9yt52pdq9e7ui7Pae51eW4Kp97VdXlOC6eeJ7ZZUupdXbckPaeF6crjGmPyLmhKuJ9//tlIMps3b3ZaPmrUKHP77bfnWGf8+PFGEhMTExMTE5MFpqSkpDyzQonv2XHHuHHjNHLkSMd8VlaWTp48qXLlyslmsxX59tPS0hQREaGkpCQFBQVZrl5JamtJqVeS2spzU/LrlaS28tyU/HoFYYzRmTNnVKlSpTzLlfiwc9NNN8nLy0spKSlOy1NSUhQeHp5jHT8/P/n5+TktCwkJKaom5iooKMitF0RJqeeJbVq9nie2WVLqeWKbVq/niW2WlHqe2KbV67krODj4umVK/NVYvr6+atasmdatW+dYlpWVpXXr1ikyMtKDLQMAAMVBie/ZkaSRI0eqX79+at68uW6//XZNnz5d586dc1ydBQAA/rwsEXbuv/9+/frrr3ruueeUnJysJk2aaNWqVQoLC/N003Lk5+en8ePHZzuUZpV6ntim1et5YpslpZ4ntmn1ep7YZkmp54ltWr3ejWAz5nrXawEAAJRcJf6cHQAAgLwQdgAAgKURdgAAgKURdgAAgKURdgAAgKURdoqhGjVq6Pfff/d0MwDA4ezZs55uAorApk2brltm2LBhN6AlRYuwcwMYY/T111/rww8/1JIlS7Rz584879B69OhRZWZmFklb0tLSNGvWLDVv3rxIHt8V586dc6te06ZNdeutt153utakSZP0/fff5/q4aWlpeuihh9xqU27+LB8k13P+/PnrljHGaP369Vq5cqVOnTpVoO19+OGHBap/tYkTJ+qPP1y7o7QkLViwQBkZGYXWjqL06quv5rn+zJkzio6Ozrbc3efmenbv3i1fX99Cf1x3tllSfnxOnTrV6X321VdfOb3+zpw5o8GDB2er17VrV+3atSvXxx02bJgSEhLcalNWVpZWrFjhVt1CVwg3Hkce1q9fb6pXr27sdrux2WzGZrMZu91ubr75ZrNx48Yc69hsNpOSklLo7XjwwQdNqVKlTMWKFc3gwYOzlVm8eLHJyMhwzCclJZnMzEzH/Llz58yUKVOy1Tt8+LAZMGCAYz4iIsKULVvWMd10003mwIED2erVqFHD/Pe//3V5XyZMmOCYxo8fb3x9fc3jjz/utHzChAnZ6tlsNhMaGmrWrFmT4+MmJycbu92e47opU6aYP/74wzH/5ZdfmvT0dMd8WlqaGTRoULZ6wcHB5n//+1+u+zJ06FBTpkyZQtver7/+ao4ePeq07NtvvzX9+/c39957r3nvvfdybEePHj3yNbkqPT3dvPzyyyYsLMxp+alTp0zfvn1NgwYNzMMPP2xOnz5tWrVq5XiPhIWFmW+++SbXx7148aLZs2ePOXjwoNPyZcuWmUaNGhlfX99sdTp27GhSU1Md83FxcebUqVOO+d9++83UrVs3Wz273e7W+9HdeldkZmaauXPnmpiYGFO/fn3ToEED06VLF5OQkGCysrJyrOPuPvr7+5uEhIQcH/Ps2bPmr3/9q6ldu3a2dQXdx9zs2rUr1/fiFX/88Yf5+OOPzbRp08y0adPMxx9/7PSeKaxtFvTzeNu2bWbEiBEmJibGxMTEmBEjRpjt27fnWv7ixYtO73VjLn82TZgwwYwaNSrXz8xr/xdlypQx33//vdNj5LR/Tz75pAkLCzOHDh3Ktu7xxx83pUuXNhs2bLjufl7t0KFDZty4caZixYrG29vbpbpFhbBThA4dOmRKlSpl2rRpY5YtW2YOHDhg9u/fb5YsWWLuuusuU7p0aacX4xU2m80sWLDAfPzxx3lO1/PTTz+ZF154wdx8882mXLlyxm63m0WLFuX6Qenum+WJJ54wY8eOdcwHBgaaqVOnmvj4eBMfH286duxoHnvssWz1Ro0aZXx8fMxTTz3lFLJcFRgYmOPzeC2bzWYGDBhgfHx8zL/+9a9s6/MKOzf6g8Td7fXq1cuMHDnSMZ+SkmLKli1r6tevb7p27Wp8fHzMggULstXr379/vqacpKenm7Fjx5pmzZqZyMhI89FHHxljjJk3b56pWLGiqVy5spk8ebJTnYEDB5patWqZF154wbRo0cJERkaali1bmi1btpht27aZ1q1bm86dO+e4vT179piqVasau91u7Ha76dGjh0lOTjZ/+9vfTGhoqBkzZoxJSkoqtOfU3S+7gnxJZmVlmZiYGGOz2UyTJk1Mr169zP33328aNWpkbDab6datW4713N3HDz74wPj7+2f7XDl79qxp1aqVqVWrlvnll18KdR/zcr2w8/HHH5vy5cs7wvGVqXz58uaTTz4p1G0WZB9HjRplbDabKVOmjGncuLFp3LixCQwMNHa73YwePTrHOv379zePPvqoYz4tLc1ERESY8uXLm0aNGhlvb2+zcuXK67bz2s/FvD7fBgwYYKpWrWp+/vlnx7InnnjClCpVyqxfvz5f+/rHH3+YhIQEc+eddxq73W7uuusuM2vWLJOcnJyv+kWNsFOEhgwZYtq2bZvjuqysLNO2bVszdOjQbOuufQPnNOX1QfDhhx+ajh07mtKlS5t77rnHLFu2zGRkZBhvb2+zd+/eXOu5+2Zp0KCB2bp1a671NmzYYGrWrJnjNhMTE03dunVN/fr1zc6dO3NtW17yG3aufBH8+9//NqVKlTL9+vVzCll5fRjc6A8Sd7dXrVo1p/A0bdo0c/PNN5uLFy865lu0aJGt3vfff+/Ui+eK0aNHm+DgYBMbG+v4JffII4+Yhg0bmvfff99cunQpW51KlSo52vnTTz8Zm81mvvjiC8f6rVu3ZusNuqJTp07m7rvvNsuXLzd///vfjc1mM3Xq1DHTpk3L85e9u8+pzWYzJ06cuO7zUFj1jLkcFMuUKZPj62PdunWmTJkyOfbEFOR1OmfOHFOqVCnH/+Hs2bPmjjvuMDVr1nR67V67PXf3MS95hZ2vvvrK+Pj4mNjYWLN582Zz6tQpc+rUKfPVV1+Znj17Gl9fX5OYmFho23T3x2d8fLzx9/c3r7/+urlw4YJj+YULF8xrr72Wa29arVq1zOrVqx3zb7zxhqlUqZKjx2706NGmdevWObbT3f99Zmam6dGjh6lbt6757bffzIgRI0xAQIBZu3ZtjuWvtm3bNvPoo4+aoKAg07RpU/Pyyy8bLy+vPL9rPIGwU4Tq16+f56+MTz75xNSvXz/b8oL+WvLy8jL//Oc/TVpamtPyogo7gYGBTr+khw8fbn777TfH/NGjR42/v3+u201PTzdPPfWU8ff3N126dHH50IkrPTtX9u/rr782VapUMS1atHD8Yi2qsOPOB4m72/P393c6jNWxY0czatQox/zBgwdNaGhotnrX9gjcd999+f5FVr16dceH/Z49exw9aLn1IBpz+TV6dU9BQECAOXz4sGP++PHjuT6f5cuXdxwaTE1NdXwZXU9Bwk5ISIjTodmcppzqNWzY0DRt2jTPKSft2rUzcXFxue7Liy++aNq3b19o+3jFlClTTFBQkPniiy/MnXfeaWrUqJFjL9nV23PnuTl9+nSe03//+99c29mxY0enno9rPfroo6Zjx46Ftk13f3zedtttOfYgX/HKK6+Y2267LdvyUqVKmR9++MEx36NHDzNs2DDH/N69e0358uVzbGdB/vcZGRkmKirKlC9f3pQqVSrXw/1Xa9iwoalataoZN26c+fbbbx3Lr/dd4wmWuBFocXXs2DE1bNgw1/UNGjTQjz/+mG25zWa77mN/++23atCgQY7rBg4cqJkzZ2rDhg3q06eP7r//fpUtWzb/DXeR3W7XL7/8osqVK0vKfrJjSkqKfHx8cq2fkZGhEydOyGazKTg4WN7eRf+ybNasmbZv36577rlHzZs319KlS1WtWrUi2ZbdbteiRYsUExOjunXr6ty5c/rkk0909913F/q2goKClJqaqqpVq0qStm3bpoEDBzrW22y2HE+aNdecMP/pp58qLi4uX9v86aef1KxZM0mXX9N+fn4aMWJEnq/jrKwseXl5Oea9vLycyudV97ffflOlSpUkScHBwSpdurRatmx53XbabLZsj5uf95okPf/88woODs5X2atFR0crMDDQ5Xq7d+/W1KlTc13fsWNHzZgxI9vyguyjJI0ePVonT57U3XffrWrVqmnDhg2O93Vu3HluQkJC8myXMSbX9Vu2bNGUKVNyrTtkyBDdddddhbrN5ORkVahQIde6Odm7d6+6deuW6/ru3bvr2Wefzbbc39/f6UTjLVu2aNq0aU7rc7sy7p133nG83i5duqT4+HjddNNNki6foJyTq19HrVu31n//+19FR0dr37592rdvn2Pd448/nq3uwYMHdf/996tNmzaqV69ervtaHBB2itDZs2dVqlSpXNeXKlUqxysZrv3iueLMmTN6//339c4772jHjh25XrH11ltvafr06frPf/6jefPmafjw4YqOjpYxRllZWXm2efXq1Y4PrqysLK1bt07ffvutJCk1NTXHOvXr19fatWt1++235/qYuQWzNWvW6KGHHlLFihW1Y8cO1a1bN8/2Scr2IX/tm/qKa9+c136QVahQQevXr9ewYcPUunVrPffcc3lu90Z/kLizvZYtW2rGjBmaM2eOli5dqjNnzqht27aO9d99950iIiLy3E9XZWZmOl3F4u3tna8veHf2T7r8fzxz5oz8/f0dX1Dnz59XWlqaU7mgoCCneWOM+vfv77gjc3p6uv7xj3+odOnSkpTnlVO9evVy+ctOkkaNGpVrvZ9++kkTJ07Mcd3JkycVFhaW6+OGhYXleMWau/vYs2dPp3kfHx/ddNNNeuKJJ5yWL126NFtdd56b9evXuxTCrnb+/Pls/9urBQcHKz09PdvyL774wq3tucvLy0sXLlzIdf3FixedAv8VTZo00bvvvqu4uDj997//VUpKitN7+Pvvv3eE/atVqVJFc+bMccyHh4fr3XffzVbmWtf+OK1YsaJ2796t3bt3O5bZbLYcP6N++OEHxcfHa9CgQTp//rweeOAB9e7d2+3/bVHirudFyG63a/369QoNDc1x/W+//aZ27dplCy0DBgzQjBkzVKZMGUmXL1+eO3eulixZokqVKqlnz56KjY3Vbbfdlq92HDp0SPPmzdOCBQt09uxZxcTE6J577sn2AWe3X38kApvNlq29c+bM0fDhw/Wf//xHMTExTuuWL1+uXr16afr06XrkkUec1j322GOKj4/X008/raeffjrHN35Oqlevnq92/vDDD07L7HZ7rr/Q3n77bT3++OO6ePFijiGyWrVq+XoDHzlypFDa6u72vvnmG0VFRSktLU2XLl3SuHHj9MILLzjW9+nTR6VKldJbb73lVM/Ly0vJyckqX768JKlMmTLavXt3vtpvt9vVsWNHxxfs8uXL1bZtW8cX7BVXf1G6u39Xtnd13Wt/kV+Zv/b/2L9//3xtc/78+dm2584vey8vLx0/fjzXet98841uvfXWHF9v1/4/rpWSkqJKlSoV2j4OGDDgunVyqne9fSwKjRo10ogRI3Jt87x58zR9+nSnL2vp8o+3adOm6ZNPPtGFCxd09913a/z48QoICMhze+7+/1u3bq0777xTkyZNynH9M888oy+//FIbNmxwWr5x40Z17NhRFStW1PHjx/XAAw9o7ty5jvWDBw/W2bNntWDBApfaU9TWr1+vefPmaenSpUpPT9dTTz2lhx9+WLfccounmyaJsFOkrnwo5/UU5/ShLF3uNo2Pj9fcuXOVlpam++67T7Nnz9Y333zjdndhVlaWPv30U73zzjv67LPPCnUMkAceeECLFy9WnTp1VLt2bUmXuzgPHDige+65R//5z3+y1WnQoIHeffddValSReXKlZMkJSUlac6cOTp//ry6dOmiv/3tb4XWxueff16jRo3Ktbdt8+bNeueddzRv3rxC26Yn/Pbbb/rqq68UHh6uFi1aOK1buXKl6tevn+2QnTuB5Qp3v2DdtXHjxnyVy+lQhjvsdrtSUlJyDR551cvrSzKvsHPt/+NaGRkZWrVqVZGNx5VfBXlurveasdlsunTpUrblr776ql544QW9++676tSpk9O6lStXql+/fvrnP/+pkSNHOq2bNGmSJkyYoKioKAUEBGj16tV64IEHrvt+79Onj2655RatWrXKpZC0YsUKde/eXSNHjtSTTz7p6KlLTk7WK6+8ounTp+ujjz5S586ds9Xdt2+f1qxZo/DwcN17771OP0TffvttNWzYUJGRkU51EhMT9fvvvzs93oIFCzR+/HidO3dO3bt31+uvv57tNeVuvdykpqZq4cKFmjdvnnbu3KkGDRpkC56eQNgpQjmdj3OtM2fOZDvE06VLF23atEkxMTHq3bu3OnToIC8vL/n4+LgUdn7//fdcQ0SdOnVy/RDOqV56erq6dOmiO++8M9ftLVq0SO+//74OHTokSapVq5YeeOAB9erVK8fyu3fvVteuXZWUlKRatWpp0aJF6tChg86dOye73a5z587pww8/VPfu3Z3qrV+/XkOHDtWWLVuydWefPn1af/3rXzV79uxsbc1PvVmzZuUasLKyshQfH6+lS5fq6NGjstlsqlGjhmJjY9WnT59cP7zdqdepUye9//77jkOKkydP1j/+8Q+FhIRIuvw/uvPOO50OhRWknru/7KXLXdnVqlXLV8/gFenp6Vq7dq3jA3bcuHFO4dvb21sTJ06Uv79/trrXHq7KzbX/42t7MnNis9m0ZMkSp2U9evTIV5i7Ngj++OOPioiIyPV5ySvsuBsg8zMops1mc+opKEi9AQMG5Kud1waKjz/+ONeyiYmJmjFjhrKysnI8HJWVlaX7779fS5YsUe3atVW3bl0ZY7R//34dOnRI3bt31wcffJDtea9Vq5aeeuopPfbYY5KktWvXKiYmRufPn8/ztetuSJKk119/XU899ZQuXbrkeE+ePn1a3t7emjp1arbDhNeTkZGhN954Q9OmTVNycrLTug4dOqhNmzYaM2aMJGnPnj269dZb1b9/f9WtW1fTpk3TY489pgkTJhRKvfzYtWuX5s2bl+P5ZTcaYccDrpx7M3fuXH399dfZPuy8vb31+OOPa9CgQapVq5ZjeX7Dzp49e9SlSxeXQ4S79a64OiQdO3ZM77zzjs6fP6+uXbvmGJI6deokLy8vjR07Vu+++65WrFih6Ohox3HnYcOGaceOHdqyZYtTva5du6pNmzYaMWJEju2YMWOGvvjiC3300UdOy7t166bWrVu7XE+6fGikc+fO+uyzz9S4cWPVqVPH8QG7Z88ede3aVcuWLSu0etf2CgQFBWnXrl2qUaOGpNwPY1x7WCG/9Qri2m3ef//9mjFjRp7nnMyePVsrV67U8uXLJV0+bFa/fn3Hr+UDBw5o1KhR2X6dS/nrFZCU4+Hh/CisQzzXk1fYcZfdblfVqlXVtGnTPHuUr32N3+h6OTl48KDGjh2r5cuXq3fv3po4caLjRPucLF68WAsXLnT8uLrlllvUq1evXH9c+fn56fDhw07nrPn7++vw4cN5noR9yy236Mknn3Q5JF2RlJSkDz/80KmdsbGxioiI0Pnz57P1EGVkZGjChAlas2aNfH19NXr0aHXv3l3z5893HO4fOnSoI5xcUbFiRS1fvtwxOv7TTz+tjRs36ssvv5QkffDBBxo/fny2Hzru1pOksmXL5vheDA4O1i233KKnnnpK7dq1u+5zdEPcoKu+YIzZuHGj6du3ryldurSpVauWGTNmjNm2bVu2comJiebhhx82ZcqUMbfffrt5/fXXza+//prvy/k6dOhgOnfubL788kvz2GOPmb/85S/moYceMpmZmSYzM9MMHjw4x7FW3K23e/duxyBvtWvXNv/73/9MWFiYCQwMNEFBQcbLy8sx0NzVypUr5xgl98yZM8Zms5mvv/7asX7//v0mODg4W70qVaqYffv25br/+/fvNxEREdmWR0REuFXPGPfHPbnR46UU9PJTd1xvmzm54447nIZluLbOu+++a1q2bJlj3Q0bNjimL774wgQEBJj33nvPabmrI74WheuNRt2mTZs8x2a63vTQQw9lqzd48GBTtmxZ06RJE/Paa6+Z33//PV9tvdH1rvbzzz+bhx9+2Pj4+JjOnTubPXv2uPwY+WG327ONCRQYGOh0mXdOfH19zbFjx5yW+fn55XlJ/vWkp6ebV155JcexpNwZt+pKm65uZ6tWrcwLL7zgmD9y5IgJDAwstHrGGMfAsddO06dPN3369DG+vr5uD/JY2Ag7Rez48eMmLi7O1KxZ01SoUMEMHTo036Hl7NmzZu7cuaZVq1bGx8fH2O12M3369Gzj51zL3RDhbr2cQtKAAQOuG5Lc/WL28/PLcUTiKw4dOpTjuD7u1jPG/XFPbvR4KSUl7ISHh5sjR4445m+66San+YMHD5qgoKB8bT+/4yzdaAUZldpms5lq1aqZHj16mO7du+c65SQ9Pd0sXLjQREVFmVKlSpl7773XrFq1Ks9xjzxRLzU11YwePdoEBASYyMhIs2nTpjzLX3FlXJu8Ji8vrxzrderUySlwent7m/bt2+c5rpe7IcmdkcWNcW/cKmMu/wi8cguijIyMbGN57d69O8cxj9ytlx+vvPKKiYyMdKtuYSPsFKHOnTuboKAg88ADD5gVK1Y4Erk7Ay4dOHDAjBo1yoSHhzsG38vNjf6idDckXTv66rUfILltr0aNGjn2FF2xZMkSU7169UKrZ4wxYWFhed7jaufOnTn+SnO33rUfsPl9btytVxDX22ZO/P39c7xf2hX79+83fn5++dp+cQ07BVEYPSbGXB7Qc8KECaZGjRqmSpUq5syZM8Wi3pQpU0xoaKipV6+eWbZsWb73x5jL9z/LbRozZowJCAjI8bXjbvh0NyS520Pj4+NjfvrpJ8e8v7+/2b1793Wfl3/84x+O0Dhy5EhTrlw5pxHi//3vf5vmzZsXWr38OHjwoNtBqbAxzk4R+uyzz3I898YdtWvX1tSpUxUXF6fly5df9+Q4dwcWc6feyZMnFR4eLkkKDAxU6dKlnQYxLFu2bK7jprgzJkinTp307LPPqkOHDtlOYD1//rzGjx+f4xUO7ta7so/ujHtyo8dLcbdeQVxvm1dcfQJv5cqV9e233zqu3LvW7t27rzuYnZXNnDlT//rXv7R06VLNmzdP48aNU0xMjAYOHKj27dvn+/189RWhrpwbVNT1xo4dq4CAANWsWVMJCQm53lU7p6v/chqoL6fzfa7l7tWA/fr1y7bswQcfvG69Dz74QAsWLFDXrl317bffqlGjRrp06ZK++eabPP9/7o5bNWnSJPXs2VN33XWXAgMDlZCQ4PQ48+bNU/v27QutXn5kZGTc8LvX54YTlIvQli1bNHfuXC1evFh169ZVnz591KtXL1WsWLFAl5Bfz/UuI87tstWC1Lv68tNrx2jJ7aRYd0/8TElJ0a233uo4Ue/KF+aBAwc0c+ZMZWZmaufOndlChrv1JPfHPXG3XnE7mTYv7mzziSee0Nq1a7Vjx44cg2fz5s0VFRWl11577bqP68qYQCXVjz/+qPj4eC1YsECXLl3S3r17c/0CzMjIcISkL7/8Up07d9aAAQPUoUOHPE+ovZH1Cmu4gl9++UXjx49XQkKCoqOjFRcXl+sApjear6+vjhw5or/85S+SpICAAG3bti3PUfWlgg0DIV2+2iswMDDbuGUnT55UYGBgruHD3Xp5GT58uA4cOKBVq1a5XLewEXZugHPnzmnx4sWaN2+etm3bpszMTP3rX//SQw895Bg4sDDd6C9Kd0NSQfz4448aNGiQVq9e7bgKxGazKTo6WjNnzsz1i8/deu6Oe1JSxku50VJSUtSkSRP5+vpq6NChjoHHDh48qDfeeEOXLl3S//73vxyD57WXkLv6ZVASJSUlaf78+YqPj9eFCxd04MCBHMPO4MGDtWjRIkVEROihhx5S7969s40snpMbXa+gTp8+rZdeekmvv/66mjRpoilTpuQ5LIYnuDtQpyd+sLgrp6slpcv/n507d+q7777Tpk2bHLeT8STCzg128OBBzZ07V++++65SU1PVrl07ffLJJ55uVoF48s156tQpHT58WMYY1apVK9/3AHO1XknqaSkpjhw5okGDBmnNmjVOwbNdu3Z68803HZfLX+vP8py602Nit9tVpUoVNW3aNM+ek2uD4I2uVxBTp07VlClTFB4erpdeeinP+095UkF7aEqCNm3a5Lg8KChItWvX1qBBg4pNjythx0MyMzMd596U9LADFMTJkyd1+PBhSVLNmjVzvb3Kn4m7PSbuHh660fUKwm63KyAgQFFRUXneYsbTIeLPEspLCsIOABQznugxKSk8EbBQ8nE1FgAUM3379i2Wd44uDuLj4z3dBJRA9OwAAABLy/+d+wAAAEogwg4AALA0wg4AALA0wg4AALA0wg4AALA0wg4Aj0pOTtawYcNUo0YN+fn5KSIiQl26dNG6detuaDtsNpuWLVt2Q7cJ4MZgnB0AHnP06FG1atVKISEhmjZtmho2bKiLFy9q9erVGjJkiA4cOODpJjq5cOFCsbmLM4D8o2cHgMcMHjxYNptN27ZtU2xsrG655RbVr19fI0eO1JYtWyRJx44dU7du3RQYGKigoCDdd999SklJcTxG//791b17d6fHHT58uFq3bu2Yb926tR5//HGNHj1aoaGhCg8P14QJExzrq1WrJknq0aOHbDabY37ChAlq0qSJ3nnnHVWvXl3+/v5asGCBypUrp4yMDKdtdu/eXX369Cm05wZA4SHsAPCIkydPatWqVRoyZEi2myNKUkhIiLKystStWzedPHlSGzdu1Jo1a/TDDz/o/vvvd3l7CQkJKl26tLZu3aqpU6dq4sSJWrNmjSRp+/btki7fYuD48eOOeUk6fPiwlixZoqVLl2rXrl269957lZmZ6XRPuxMnTmjlypV66KGHXG4XgKLHYSwAHnHlrvN16tTJtcy6deu0Z88eHTlyRBEREZKkBQsWqH79+tq+fbtuu+22fG+vUaNGGj9+vCSpVq1aeuONN7Ru3Tq1a9dO5cuXl3Q5YIWHhzvVu3DhghYsWOAoI0l///vfNX/+fN17772SpH//+9+qUqWKU28SgOKDnh0AHpGfO9Xs379fERERjqAjSfXq1VNISIj279/v0vYaNWrkNF+xYkWdOHHiuvWqVq3qFHQk6ZFHHtHnn3+un3/+WdLl+zXl9waVAG48enYAeEStWrVks9kKfBKy3W7PFpwuXryYrZyPj4/TvM1mU1ZW1nUfP6dDbE2bNlXjxo21YMECtW/fXnv37tXKlStdbDmAG4WeHQAeERoaqujoaM2cOVPnzp3Ltj41NVV169ZVUlKSkpKSHMv37dun1NRU1atXT5JUvnx5HT9+3Knurl27XG6Pj4+PMjMz813+4YcfVnx8vObPn6+oqCin3icAxQthB4DHzJw5U5mZmbr99tu1ZMkSHTp0SPv379eMGTMUGRmpqKgoNWzYUL1799bOnTu1bds29e3bV3fddZeaN28uSWrbtq2+/vprLViwQIcOHdL48eP17bffutyWatWqad26dUpOTtapU6euW/7vf/+7fvrpJ82ZM4cTk4FijrADwGNq1KihnTt3qk2bNnryySfVoEEDtWvXTuvWrdOsWbNks9n08ccfq2zZsvrb3/6mqKgo1ahRQ4sXL3Y8RnR0tJ599lmNHj1at912m86cOaO+ffu63JZXXnlFa9asUUREhJo2bXrd8sHBwYqNjVVgYGC2S98BFC82k5+zBAEA2dx9992qX7++ZsyY4emmAMgDYQcAXHTq1Clt2LBB99xzj/bt26fatWt7ukkA8sDVWADgoqZNm+rUqVOaMmUKQQcoAejZAQAAlsYJygAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNL+D7tJ9B3e1ENhAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "European countries that are not in the dataset: \n",
      "['AL', 'GR', 'VA', 'HU', 'IS', 'LV', 'LI', 'LU', 'MC', 'SM', 'SI']\n"
     ]
    }
   ],
   "source": [
    "display_number_of_images_in_each_folder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also plot the report of number of images over the total country surface to observe any imbalances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "european_country_surface_areas = {\n",
    "    'AL': 28748,  # Albania\n",
    "    'AD': 468,    # Andorra\n",
    "    'AT': 83879,  # Austria\n",
    "    'BY': 207600, # Belarus\n",
    "    'BE': 30528,  # Belgium\n",
    "    'BA': 51197,  # Bosnia and Herzegovina\n",
    "    'BG': 110879, # Bulgaria\n",
    "    'HR': 56594,  # Croatia\n",
    "    'CY': 9251,   # Cyprus\n",
    "    'CZ': 78865,  # Czech Republic\n",
    "    'DK': 42924,  # Denmark\n",
    "    'EE': 45227,  # Estonia\n",
    "    'FI': 338424, # Finland\n",
    "    'FR': 551695, # France\n",
    "    'DE': 357022, # Germany\n",
    "    'GR': 131957, # Greece\n",
    "    'HU': 93030,  # Hungary\n",
    "    'IS': 103000, # Iceland\n",
    "    'IE': 70273,  # Ireland\n",
    "    'IT': 301340, # Italy\n",
    "    'LV': 64589,  # Latvia\n",
    "    'LI': 160,    # Liechtenstein\n",
    "    'LT': 65300,  # Lithuania\n",
    "    'LU': 2586,   # Luxembourg\n",
    "    'MK': 25713,  # North Macedonia\n",
    "    'MT': 316,    # Malta\n",
    "    'MD': 33843,  # Moldova\n",
    "    'MC': 2,      # Monaco\n",
    "    'ME': 13812,  # Montenegro\n",
    "    'NL': 41543,  # Netherlands\n",
    "    'NO': 1487290,# Norway\n",
    "    'PL': 312696, # Poland\n",
    "    'PT': 92212,  # Portugal\n",
    "    'RO': 238397, # Romania\n",
    "    'RU': 17098242,# Russia\n",
    "    'SM': 61,     # San Marino\n",
    "    'RS': 77474,  # Serbia\n",
    "    'SK': 49037,  # Slovakia\n",
    "    'SI': 20273,  # Slovenia\n",
    "    'ES': 505992, # Spain\n",
    "    'SE': 450295, # Sweden\n",
    "    'CH': 41284,  # Switzerland\n",
    "    'UA': 603500, # Ukraine\n",
    "    'GB': 243610, # United Kingdom\n",
    "    'VA': 0.44,   # Vatican City\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_density_of_images_per_country():\n",
    "    filtered_european_country_surface_areas = {key: european_country_surface_areas[key] for key in country_nr_images.keys()}\n",
    "\n",
    "    for country_code in country_nr_images.keys():\n",
    "        filtered_european_country_surface_areas[country_code] = country_nr_images[country_code] / filtered_european_country_surface_areas[country_code] * 100\n",
    "\n",
    "    plt.bar(country_nr_images.keys(), filtered_european_country_surface_areas.values())\n",
    "    plt.xlabel(\"Country\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel(\"Nr of Images\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_density_of_images_per_country()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important cell to run!**\n",
    "\n",
    "We save the image data into a pandas dataframe `all_countries_df` containing the path to the image, the latitude and the longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>long</th>\n",
       "      <th>lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c:\\Facultate\\An_4\\An_4_Sem_1\\Pattern_Recogniti...</td>\n",
       "      <td>1.87154</td>\n",
       "      <td>42.02136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c:\\Facultate\\An_4\\An_4_Sem_1\\Pattern_Recogniti...</td>\n",
       "      <td>1.88966</td>\n",
       "      <td>42.02381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c:\\Facultate\\An_4\\An_4_Sem_1\\Pattern_Recogniti...</td>\n",
       "      <td>1.88094</td>\n",
       "      <td>42.02883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c:\\Facultate\\An_4\\An_4_Sem_1\\Pattern_Recogniti...</td>\n",
       "      <td>1.30027</td>\n",
       "      <td>42.04606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c:\\Facultate\\An_4\\An_4_Sem_1\\Pattern_Recogniti...</td>\n",
       "      <td>1.84825</td>\n",
       "      <td>42.07719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path     long       lat\n",
       "0  c:\\Facultate\\An_4\\An_4_Sem_1\\Pattern_Recogniti...  1.87154  42.02136\n",
       "1  c:\\Facultate\\An_4\\An_4_Sem_1\\Pattern_Recogniti...  1.88966  42.02381\n",
       "2  c:\\Facultate\\An_4\\An_4_Sem_1\\Pattern_Recogniti...  1.88094  42.02883\n",
       "3  c:\\Facultate\\An_4\\An_4_Sem_1\\Pattern_Recogniti...  1.30027  42.04606\n",
       "4  c:\\Facultate\\An_4\\An_4_Sem_1\\Pattern_Recogniti...  1.84825  42.07719"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coordinates_data = {\n",
    "    'path': [],\n",
    "    'long': [],\n",
    "    'lat': []\n",
    "}\n",
    "\n",
    "for country_folder in os.listdir(DATASET_ROOT_PATH):\n",
    "    if (country_folder in europe_countries_code_list):\n",
    "        curr_path = os.path.join(DATASET_ROOT_PATH, country_folder)\n",
    "        for filename in os.listdir(curr_path):\n",
    "            curr_lat, curr_long = filename[: -4].split(',')\n",
    "            coordinates_data['path'].append(os.path.join(curr_path, filename))\n",
    "            coordinates_data['lat'].append(float(curr_lat))\n",
    "            coordinates_data['long'].append(float(curr_long))\n",
    "\n",
    "all_countries_df = pd.DataFrame(coordinates_data)\n",
    "all_countries_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the geographic distribution of images (folium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_map_with_markers(path):\n",
    "    my_map = None\n",
    "    map_center = [45, 15]\n",
    "    my_map = folium.Map(location=map_center, zoom_start=5)\n",
    "\n",
    "    # Add markers for each coordinate\n",
    "    for coordinates in list(zip(all_countries_df.lat, all_countries_df.long)):\n",
    "        folium.CircleMarker(location=(coordinates[0], coordinates[1]), radius=1).add_to(my_map)\n",
    "\n",
    "    # Save the map as an HTML file\n",
    "    my_map.save(path)\n",
    "    return my_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = create_map_with_markers(\"./maps/map_with_markers.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What kind of data is needed in the model?\n",
    "#### Data preprocessing\n",
    "Preprocessing is strongly tied to the choice of the model used (CNN / Vision transformer / Deep neural networks), therefore it may be adapted later in the implementation\n",
    "\n",
    "Let us first prepare the input data for being fed into a Convolutional Neural Network. This type of network expects a 4D Tensor. For a colored image, this tensor would have the shape\n",
    "```\n",
    "[BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WIDTH, NUMBER_OF_CHANNELS]\n",
    "```\n",
    "\n",
    "The images will be preprocessed as follows:\n",
    "- resize to the STANDARD_SIZE in this case 256 pixels for an edge (square shape)\n",
    "- normalize the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV understanding on how images are processed\n",
    "example_img = cv.imread(os.path.join(dataset_rel_path, \"CH/45.72656,6.52431.jpg\"))\n",
    "print(type(example_img))\n",
    "print(np.shape(example_img))\n",
    "\n",
    "# resize\n",
    "resized = cv.resize(example_img, (STANDARD_IMAGE_SIZE, STANDARD_IMAGE_SIZE), interpolation=cv.INTER_NEAREST)\n",
    "print(np.shape(resized))\n",
    "print(resized[0][0]) # print the color channels B G R\n",
    "\n",
    "cv.imshow(\"original\", example_img)\n",
    "cv.imshow(\"resized\", resized)\n",
    "cv.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Labeling, creating dataset, splitting into test, train, validation sets\n",
    "We need to get from the current state of the data to the data suitable for training, validation and testing.\n",
    "- *current* state: jpg images aranged into folders and labeled with their latitude and longitude values \n",
    "- *desired* state: individual examples consisting of numpy arrays of the required shape, populated with the RGB channels values, and labeled with an integer corresponding to the area on the map where they belong.\n",
    "\n",
    "Data Structures for the Images:\n",
    "- x_train, x_validate, x_test: arrays containing the pixel data of the images (will be fed into the neural network)\n",
    "- y_train, y_validate, y_test: arrays containing the label of each example.\n",
    "\n",
    "> Note: The label and the example need to be at the same index in the data structures that hold them. Therefore, they need to be shuffled together\n",
    "\n",
    "##### Obtaining the output labels - (Method 1) **Variable-sized rectangles in a grid** ‚èπ\n",
    "In this method the map of europe will be divided into square shapes, therefore the label of an image is the index of the square into the ordered list of squares. The size of the square is determined by the number of training examples in that square. Therefore if it exceeds the `THRESHOLD` it will be split into 4 (its size is reduced). Obtaining these squares is a subproblem on its own and will be solved in the following part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "INITIAL_SQUARE_EDGE_SIZE = 10\n",
    "POINTS_IN_SQUARE_TH = 300\n",
    "\n",
    "all_countries_labeled = []\n",
    "\n",
    "def isInBbox(point_coords, bottom_left, size):\n",
    "    if (point_coords[0] > bottom_left[0] and point_coords[0] < bottom_left[0] + size):\n",
    "        if (point_coords[1] > bottom_left[1] and point_coords[1] < bottom_left[1] + size):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def updateLabelForElement(countries_labeled, new_point):\n",
    "    for index, element in enumerate(countries_labeled):\n",
    "        if element['path'] == new_point['path']:\n",
    "            countries_labeled[index]['label'] = new_point['label']\n",
    "            break\n",
    "    return countries_labeled\n",
    "\n",
    "def getSquaresFromArea(countries_labeled, bottom_left, size, points_df, len_squares):\n",
    "    squares_list = []\n",
    "    squares_list_len = 0\n",
    "\n",
    "    bottom_left_x = bottom_left[0]\n",
    "    while bottom_left_x < (bottom_left[0] + 2 * size):\n",
    "        bottom_left_y = bottom_left[1]\n",
    "        while bottom_left_y < (bottom_left[1] + 2 * size):\n",
    "            count_points = 0\n",
    "            curr_points_df = []    \n",
    "            for index, point in all_countries_df.iterrows():\n",
    "                if isInBbox((point['long'], point['lat']), (bottom_left_x, bottom_left_y), size):\n",
    "                    count_points += 1\n",
    "                    curr_points_df.append(point)\n",
    "                    new_point = {\n",
    "                        'path': point['path'],\n",
    "                        'label': len_squares + squares_list_len\n",
    "                    }\n",
    "                    countries_labeled = updateLabelForElement(countries_labeled, new_point)\n",
    "\n",
    "            if count_points >= POINTS_IN_SQUARE_TH:\n",
    "                # break up the square even more\n",
    "                countries_labeled, squares_list_from_area = (getSquaresFromArea(countries_labeled, (bottom_left_x, bottom_left_y), size / 2, points_df, len_squares + squares_list_len))\n",
    "                squares_list_len += len(squares_list_from_area)\n",
    "                squares_list += squares_list_from_area\n",
    "\n",
    "            elif count_points > 0:\n",
    "                squares_list.append((bottom_left_x, bottom_left_y, size))\n",
    "                squares_list_len += 1\n",
    "            bottom_left_y += size\n",
    "        bottom_left_x += size\n",
    "    return countries_labeled, squares_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "europe_bbox = {\n",
    "    'lat_min': 36.03,\n",
    "    'lat_max': 71.13,\n",
    "    'long_min': -10.72,\n",
    "    'long_max': 41.31\n",
    "}\n",
    "\n",
    "squares = []\n",
    "squares_len = 0\n",
    "\n",
    "bottom_left_x = europe_bbox['long_min']\n",
    "while(bottom_left_x < europe_bbox['long_max']):\n",
    "    bottom_left_y = europe_bbox['lat_min']\n",
    "\n",
    "    while (bottom_left_y < europe_bbox['lat_max']):\n",
    "        count_points = 0\n",
    "        curr_points_df = []\n",
    "        \n",
    "        for index, point in all_countries_df.iterrows():\n",
    "            if isInBbox((point['long'], point['lat']), (bottom_left_x, bottom_left_y), INITIAL_SQUARE_EDGE_SIZE):\n",
    "                curr_points_df.append(point)\n",
    "                count_points += 1\n",
    "                point['label'] = squares_len\n",
    "                all_countries_labeled.append(point)\n",
    "\n",
    "        if count_points >= POINTS_IN_SQUARE_TH:\n",
    "            # break the square even more\n",
    "            all_countries_labeled, squares_in_area = getSquaresFromArea(all_countries_labeled, (bottom_left_x, bottom_left_y), INITIAL_SQUARE_EDGE_SIZE / 2, pd.DataFrame(curr_points_df), squares_len)\n",
    "            squares_len += len(squares_in_area)\n",
    "            squares += squares_in_area\n",
    "\n",
    "        elif (count_points > 0):\n",
    "            squares.append((bottom_left_x, bottom_left_y, INITIAL_SQUARE_EDGE_SIZE))\n",
    "            squares_len += 1\n",
    "        \n",
    "        bottom_left_y += INITIAL_SQUARE_EDGE_SIZE\n",
    "    \n",
    "    bottom_left_x += INITIAL_SQUARE_EDGE_SIZE\n",
    "       \n",
    "labeled_examples_df = pd.DataFrame(all_countries_labeled) # {path to image, label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labeled_examples_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the map with the grid (using folium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_map = folium.Map(location=map_center, zoom_start=5)\n",
    "\n",
    "for index, square in enumerate(squares):\n",
    "    folium.Rectangle(\n",
    "        bounds=[[square[1], square[0]], [square[1] + square[2], square[0] + square[2]]], # long lat (y x)\n",
    "        fill=False,\n",
    "        color='orange',\n",
    "        fill_color='orange',\n",
    "        fill_opacity=0.1,\n",
    "        popup=f'{index}'\n",
    "    ).add_to(my_map)\n",
    "\n",
    "# Add markers for each coordinate\n",
    "for coordinates in list(zip(labeled_examples_df.lat, labeled_examples_df.long, labeled_examples_df.label)):\n",
    "    folium.CircleMarker(location=(coordinates[0], coordinates[1]), radius=2, popup=coordinates[2]).add_to(my_map)\n",
    "\n",
    "\n",
    "my_map.save('./maps/map_w_rectangular_grid.html') # Remark: squares appear as rectangles due to map projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Obtaining the output labels - (Method 2) **K clusters - using K-means clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distance\n",
    "def distance(pos_from, pos_to):\n",
    "    return np.sqrt((pos_from[0] - pos_to[0]) ** 2 + (pos_from[1] - pos_to[1]) ** 2)    \n",
    "\n",
    "# K means clustering algorithm - specific for our dataset\n",
    "def kMeansClustering(k, countries_df, max_iterations, initial_centers=None):\n",
    "    if initial_centers is not None:\n",
    "        centers = initial_centers.copy()\n",
    "    else:\n",
    "        centers = countries_df.sample(n=k)\n",
    "        centers.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    membership_function = np.arange(len(countries_df))\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # save previous membership function\n",
    "        membership_prv = np.copy(membership_function)\n",
    "        # compute new membership function\n",
    "        for index, point in countries_df.iterrows():\n",
    "            min_distance = distance((point['long'], point['lat']), (centers.iloc[0]['long'], centers.iloc[0]['lat']))\n",
    "            membership_function[index] = 0\n",
    "            for index_center, center in centers.iterrows():\n",
    "                curr_distance = distance((point['long'], point['lat']), (center['long'], center['lat']))\n",
    "                if curr_distance < min_distance:\n",
    "                    min_distance = curr_distance\n",
    "                    membership_function[index] = index_center\n",
    "        # compute new centers\n",
    "        clusters_count = np.zeros(k)\n",
    "        for index, point in countries_df.iterrows():\n",
    "            cluster = membership_function[index]\n",
    "            clusters_count[cluster] += 1\n",
    "            centers.iloc[cluster]['long'] = centers.iloc[cluster]['long'] + point['long']\n",
    "            centers.iloc[cluster]['lat'] = centers.iloc[cluster]['lat'] + point['lat']\n",
    "        # normalize centers\n",
    "        for index, center in centers.iterrows():\n",
    "            centers.iloc[index]['long'] /= clusters_count[index]\n",
    "            centers.iloc[index]['lat'] /= clusters_count[index]\n",
    "        # stopping condition\n",
    "        if np.array_equal(membership_function, membership_prv):\n",
    "            break\n",
    "\n",
    "    return membership_function, centers                          \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "membership, centers = kMeansClustering(10, all_countries_df, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'centers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m rand_colors \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m available_colors \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpurple\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morange\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdarkblue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdarkgreen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlightgreen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpink\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdarkgrey\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mcenters\u001b[49m)):\n\u001b[0;32m      8\u001b[0m     rand_color \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(available_colors)\n\u001b[0;32m      9\u001b[0m     rand_colors\u001b[38;5;241m.\u001b[39mappend(rand_color)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'centers' is not defined"
     ]
    }
   ],
   "source": [
    "map_center = [45, 15]\n",
    "my_map = folium.Map(location=map_center, zoom_start=5)\n",
    "\n",
    "rand_colors = []\n",
    "available_colors = ['blue', 'green', 'purple', 'orange', 'darkblue', 'darkgreen', 'lightgreen', 'pink', 'black', 'darkgrey']\n",
    "\n",
    "for i in range(len(centers)):\n",
    "    rand_color = random.choice(available_colors)\n",
    "    rand_colors.append(rand_color)\n",
    "    available_colors.remove(rand_color)\n",
    "\n",
    "# add all points colored with random colors w.r.t. their cluster\n",
    "for index, coordinates in enumerate(list(zip(all_countries_df.lat, all_countries_df.long))):\n",
    "    folium.CircleMarker(location=(coordinates[0], coordinates[1]), radius=2, popup=index, color = rand_colors[membership[index]]).add_to(my_map)\n",
    "\n",
    "for index, center in centers.iterrows():\n",
    "    # colored with red\n",
    "    folium.CircleMarker(location=(center['lat'], center['long']), radius=2, popup='center', color = 'red').add_to(my_map)\n",
    "\n",
    "my_map.save('./maps/map_w_k_clusters.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remark: this is not the desired result (would not make sense from the geographical point of view). Another way to label the samples is to perform just the initialization step of the k_means clustering algorithm having\n",
    "some centers chosen manually (based on the distribution of the input data and on the geographical characteristics of Europe)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Obtaining the output labels - (Method 3) üëåüèΩ**Hand-picked cluster centers & 1 iteration of k-means**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_centers = pd.DataFrame({\n",
    "    'long': np.empty(6),\n",
    "    'lat': np.empty(6)\n",
    "})\n",
    "\n",
    "initial_centers.iloc[0]['long'] = -1.27 # Oxford\n",
    "initial_centers.iloc[0]['lat'] = 51.75\n",
    "\n",
    "initial_centers.iloc[1]['long'] = -3.72 # Madrid\n",
    "initial_centers.iloc[1]['lat'] = 40.39\n",
    "\n",
    "# Zadar (Croatia)\n",
    "initial_centers.iloc[2]['long'] = 15.22\n",
    "initial_centers.iloc[2]['lat'] = 44.12\n",
    "\n",
    "initial_centers.iloc[3]['long'] = 30.98 # Gomel (Belarus)\n",
    "initial_centers.iloc[3]['lat'] = 52.43\n",
    "\n",
    "# Liberec (Czech Republic)\n",
    "initial_centers.iloc[4]['long'] = 15.06\n",
    "initial_centers.iloc[4]['lat'] = 50.77\n",
    "\n",
    "# vasterviks kommun (Sweden)\n",
    "initial_centers.iloc[5]['long'] = 16.64\n",
    "initial_centers.iloc[5]['lat'] = 57.76\n",
    "\n",
    "my_map = folium.Map(location=map_center, zoom_start=5)\n",
    "for index, center in initial_centers.iterrows():\n",
    "    folium.CircleMarker(location=(center['lat'], center['long']), radius=2, color = 'red').add_to(my_map)\n",
    "\n",
    "my_map.save('./maps/map_w_handpicked_centers.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "membership, centers = kMeansClustering(6, all_countries_df, 1, initial_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_center = [45, 15]\n",
    "my_map = folium.Map(location=map_center, zoom_start=5)\n",
    "\n",
    "rand_colors = []\n",
    "available_colors = ['blue', 'green', 'purple', 'orange', 'darkblue', 'darkgreen', 'lightgreen', 'pink', 'black']\n",
    "\n",
    "for i in range(len(centers)):\n",
    "    rand_color = random.choice(available_colors)\n",
    "    rand_colors.append(rand_color)\n",
    "    available_colors.remove(rand_color)\n",
    "\n",
    "# add all points colored with random colors w.r.t. their cluster\n",
    "for index, coordinates in enumerate(list(zip(all_countries_df.lat, all_countries_df.long))):\n",
    "    folium.CircleMarker(location=(coordinates[0], coordinates[1]), radius=2, popup=membership[index], color = rand_colors[membership[index]]).add_to(my_map)\n",
    "\n",
    "for index, center in initial_centers.iterrows():\n",
    "    # colored with red\n",
    "    folium.CircleMarker(location=(center['lat'], center['long']), radius=2, popup='center', color = 'red').add_to(my_map)\n",
    "\n",
    "my_map.save('./maps/map_w_cluster_handpicked_centers.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By visualizing all three methods we notice that the best option from an intuitive perspective is the last one. Therefore, despite some of its disadvantages we take it into account. \n",
    "\n",
    "Some possible improvements for the dataset labeling are\n",
    "* forcing points that are close together (in the visible aglomerations on the map) to belong to the same cluster. (how to choose the cluster they belong to though?)\n",
    "* adding some more labels in the crowded areas (e.g UK & Ireland different from Benelux & France)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There actually exists a in tensorflow method called `image_dataset_from_directory` which creates a data set from images placed in a folder structure such that images in the same class are in the same folder. Experimentally this has been proven as a much more efficient (can't be compared) than creating a CSV with all data (its size goes up to a few GB depending on the image size).\n",
    "\n",
    "The function is a great option as it also shuffles and splits the data into train/validation partitions, transforms the class to one-hot format, and splits the dataset into batches, leveraging parallelism and memory usage efficiency.\n",
    "\n",
    "Therefore, the next cells which create numpy arrays saved in csv can and should be skipped. The commented code is left here in case one would like to load the images from csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "STANDARD_IMAGE_SIZE = 224\n",
    "# Building the labeled dataset\n",
    "x_all = np.empty((len(all_countries_df), STANDARD_IMAGE_SIZE, STANDARD_IMAGE_SIZE, 3))\n",
    "y_all = np.empty((len(all_countries_df)))\n",
    "\n",
    "for index, point in all_countries_df.iterrows():\n",
    "    image = cv.imread(point['path'])\n",
    "    image = cv.resize(image, (STANDARD_IMAGE_SIZE, STANDARD_IMAGE_SIZE), interpolation=cv.INTER_NEAREST)\n",
    "    x_all[index] = image\n",
    "    y_all[index] = membership[index]\n",
    "\n",
    "# # save the data to csv\n",
    "# x_df = pd.DataFrame(x_all.reshape(len(all_countries_df), -1))\n",
    "# y_df = pd.DataFrame(y_all.reshape(len(all_countries_df), -1))\n",
    "# x_df = x_df.astype('int32')\n",
    "# y_df = y_df.astype('int32')\n",
    "# # y_df.to_csv('./data/y_all.csv', index=False)\n",
    "# x_df.to_csv('./data/x_64_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placing images in directories corresponding to their class (the more efficient option). Also split them into test and training directories right from the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "STANDARD_IMAGE_SIZE = 224\n",
    "\n",
    "os.makedirs(f'./data/images_{STANDARD_IMAGE_SIZE}_all', exist_ok=True)\n",
    "for i in range(6):\n",
    "    os.makedirs(f'./data/images_{STANDARD_IMAGE_SIZE}_all/{i}',exist_ok=True)\n",
    "\n",
    "paths = []\n",
    "labels = []\n",
    "\n",
    "for index, point in all_countries_df.iterrows():\n",
    "    image_path = point['path']\n",
    "    paths.append(image_path)\n",
    "    labels.append(int(membership[index]))\n",
    "\n",
    "train_photos, test_photos, train_labels, test_labels = train_test_split(paths, labels, test_size=0.1, random_state=42, stratify=labels)\n",
    "\n",
    "train_directory = f'./data/{STANDARD_IMAGE_SIZE}/train'\n",
    "test_directory = f'./data/{STANDARD_IMAGE_SIZE}/test'\n",
    "\n",
    "os.makedirs(train_directory, exist_ok=True)\n",
    "for i in range(6):\n",
    "    os.makedirs(os.path.join(train_directory, f'{i}'), exist_ok=True)\n",
    "os.makedirs(test_directory, exist_ok=True)\n",
    "for i in range(6):\n",
    "    os.makedirs(os.path.join(test_directory, f'{i}'), exist_ok=True)\n",
    "\n",
    "count = 0\n",
    "for path, label in zip(train_photos, train_labels):\n",
    "    image = cv.imread(path)\n",
    "    image = cv.resize(image, (STANDARD_IMAGE_SIZE, STANDARD_IMAGE_SIZE), interpolation=cv.INTER_NEAREST)\n",
    "    final_path =  os.path.join(train_directory, f'{label}/{count}.jpg')\n",
    "    cv.imwrite(final_path, image)\n",
    "    count += 1\n",
    "\n",
    "count = 0\n",
    "for path, label in zip(test_photos, test_labels):\n",
    "    image = cv.imread(path)\n",
    "    image = cv.resize(image, (STANDARD_IMAGE_SIZE, STANDARD_IMAGE_SIZE), interpolation=cv.INTER_NEAREST)\n",
    "    final_path =  os.path.join(test_directory, f'{label}/{count}.jpg')\n",
    "    cv.imwrite(final_path, image)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in train/0: 1759\n",
      "Number of images in test/0: 195\n",
      "Number of images in train/1: 923\n",
      "Number of images in test/1: 103\n",
      "Number of images in train/2: 904\n",
      "Number of images in test/2: 100\n",
      "Number of images in train/3: 366\n",
      "Number of images in test/3: 41\n",
      "Number of images in train/4: 1228\n",
      "Number of images in test/4: 137\n",
      "Number of images in train/5: 615\n",
      "Number of images in test/5: 68\n",
      "6439\n"
     ]
    }
   ],
   "source": [
    "# verify all images were placed in folders STANDARD_SIZE/train or test/class/image.jpg sum up all images and compare to len of df with for loops\n",
    "sum = 0\n",
    "for i in range(6):\n",
    "    sum += len(os.listdir(os.path.join(train_directory, f\"{i}\"))) +len(os.listdir(os.path.join(test_directory, f\"{i}\")))\n",
    "    print(f'Number of images in train/{i}: {len(os.listdir(os.path.join(train_directory, f\"{i}\")))}')\n",
    "    print(f'Number of images in test/{i}: {len(os.listdir(os.path.join(test_directory, f\"{i}\")))}')\n",
    "print(sum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling and splitting into Test, Validation, and Training sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to divide the dataset into Test, Validation and Training set. We choose the percentages:\n",
    "* 80% training\n",
    "* 10% validation\n",
    "* 10% test\n",
    "\n",
    "For shuffling the data set in a way that ensures all labels are well represented we can take advantage of the `sklearn` method `train_test_split`\n",
    "\n",
    "> This should be run in Google Colab (or similar), as it takes a while on the local machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Neural Networks\n",
    "We will train and test multiple network architectures and observe their behavior in out classification problem. Moreover, as the required computational resources are significant we will use various image sizes, and leverage transfer learning (using pre-trained models that are fine-tuned on our data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Downloaded_Apps\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:585: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  np.object,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'object'.\n`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# import numpy as np\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# import pandas as pd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv2D, Flatten, Dense, MaxPooling2D, Input, Dropout, Lambda, Rescaling, BatchNormalization\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomFlip, RandomCrop, RandomZoom, RandomRotation, RandomTranslation, RandomBrightness\n",
      "File \u001b[1;32mc:\\Downloaded_Apps\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\__init__.py:21\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# See b/110718070#comment18 for more details about this import.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Downloaded_Apps\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\__init__.py:41\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_six\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Downloaded_Apps\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\__init__.py:46\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# from tensorflow.python import keras\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Downloaded_Apps\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\data\\__init__.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AUTOTUNE\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
      "File \u001b[1;32mc:\\Downloaded_Apps\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py:97\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m service\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dense_to_ragged_batch\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dense_to_sparse_batch\n",
      "File \u001b[1;32mc:\\Downloaded_Apps\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py:353\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m division\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[1;32m--> 353\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_dataset_id\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_dataset\n",
      "File \u001b[1;32mc:\\Downloaded_Apps\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py:26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compression_ops\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute_options\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoShardPolicy\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute_options\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExternalStatePolicy\n",
      "File \u001b[1;32mc:\\Downloaded_Apps\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\compression_ops.py:20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m division\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m structure\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_experimental_dataset_ops \u001b[38;5;28;01mas\u001b[39;00m ged_ops\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompress\u001b[39m(element):\n",
      "File \u001b[1;32mc:\\Downloaded_Apps\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwrapt\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nest\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m composite_tensor\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n",
      "File \u001b[1;32mc:\\Downloaded_Apps\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\data\\util\\nest.py:40\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_six\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sparse_tensor \u001b[38;5;28;01mas\u001b[39;00m _sparse_tensor\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_utils\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nest\n",
      "File \u001b[1;32mc:\\Downloaded_Apps\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\sparse_tensor.py:28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m composite_tensor\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constant_op\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n",
      "File \u001b[1;32mc:\\Downloaded_Apps\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types_pb2\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m execute\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m op_callbacks\n",
      "File \u001b[1;32mc:\\Downloaded_Apps\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tfe\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape\n",
      "File \u001b[1;32mc:\\Downloaded_Apps\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:585\u001b[0m\n\u001b[0;32m    556\u001b[0m     _NP_TO_TF[pdt] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m    557\u001b[0m         _NP_TO_TF[dt] \u001b[38;5;28;01mfor\u001b[39;00m dt \u001b[38;5;129;01min\u001b[39;00m _NP_TO_TF \u001b[38;5;28;01mif\u001b[39;00m dt \u001b[38;5;241m==\u001b[39m pdt()\u001b[38;5;241m.\u001b[39mdtype)  \u001b[38;5;66;03m# pylint: disable=no-value-for-parameter\u001b[39;00m\n\u001b[0;32m    559\u001b[0m TF_VALUE_DTYPES \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(_NP_TO_TF\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m    561\u001b[0m _TF_TO_NP \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    562\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_HALF:\n\u001b[0;32m    563\u001b[0m         np\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[0;32m    564\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_FLOAT:\n\u001b[0;32m    565\u001b[0m         np\u001b[38;5;241m.\u001b[39mfloat32,\n\u001b[0;32m    566\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_DOUBLE:\n\u001b[0;32m    567\u001b[0m         np\u001b[38;5;241m.\u001b[39mfloat64,\n\u001b[0;32m    568\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_INT32:\n\u001b[0;32m    569\u001b[0m         np\u001b[38;5;241m.\u001b[39mint32,\n\u001b[0;32m    570\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_UINT8:\n\u001b[0;32m    571\u001b[0m         np\u001b[38;5;241m.\u001b[39muint8,\n\u001b[0;32m    572\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_UINT16:\n\u001b[0;32m    573\u001b[0m         np\u001b[38;5;241m.\u001b[39muint16,\n\u001b[0;32m    574\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_UINT32:\n\u001b[0;32m    575\u001b[0m         np\u001b[38;5;241m.\u001b[39muint32,\n\u001b[0;32m    576\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_UINT64:\n\u001b[0;32m    577\u001b[0m         np\u001b[38;5;241m.\u001b[39muint64,\n\u001b[0;32m    578\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_INT16:\n\u001b[0;32m    579\u001b[0m         np\u001b[38;5;241m.\u001b[39mint16,\n\u001b[0;32m    580\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_INT8:\n\u001b[0;32m    581\u001b[0m         np\u001b[38;5;241m.\u001b[39mint8,\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;66;03m# NOTE(touts): For strings we use np.object as it supports variable length\u001b[39;00m\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;66;03m# strings.\u001b[39;00m\n\u001b[0;32m    584\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_STRING:\n\u001b[1;32m--> 585\u001b[0m         \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobject\u001b[49m,\n\u001b[0;32m    586\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_COMPLEX64:\n\u001b[0;32m    587\u001b[0m         np\u001b[38;5;241m.\u001b[39mcomplex64,\n\u001b[0;32m    588\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_COMPLEX128:\n\u001b[0;32m    589\u001b[0m         np\u001b[38;5;241m.\u001b[39mcomplex128,\n\u001b[0;32m    590\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_INT64:\n\u001b[0;32m    591\u001b[0m         np\u001b[38;5;241m.\u001b[39mint64,\n\u001b[0;32m    592\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_BOOL:\n\u001b[0;32m    593\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n\u001b[0;32m    594\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QINT8:\n\u001b[0;32m    595\u001b[0m         _np_qint8,\n\u001b[0;32m    596\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QUINT8:\n\u001b[0;32m    597\u001b[0m         _np_quint8,\n\u001b[0;32m    598\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QINT16:\n\u001b[0;32m    599\u001b[0m         _np_qint16,\n\u001b[0;32m    600\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QUINT16:\n\u001b[0;32m    601\u001b[0m         _np_quint16,\n\u001b[0;32m    602\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QINT32:\n\u001b[0;32m    603\u001b[0m         _np_qint32,\n\u001b[0;32m    604\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_BFLOAT16:\n\u001b[0;32m    605\u001b[0m         _np_bfloat16,\n\u001b[0;32m    606\u001b[0m \n\u001b[0;32m    607\u001b[0m     \u001b[38;5;66;03m# Ref types\u001b[39;00m\n\u001b[0;32m    608\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_HALF_REF:\n\u001b[0;32m    609\u001b[0m         np\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[0;32m    610\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_FLOAT_REF:\n\u001b[0;32m    611\u001b[0m         np\u001b[38;5;241m.\u001b[39mfloat32,\n\u001b[0;32m    612\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_DOUBLE_REF:\n\u001b[0;32m    613\u001b[0m         np\u001b[38;5;241m.\u001b[39mfloat64,\n\u001b[0;32m    614\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_INT32_REF:\n\u001b[0;32m    615\u001b[0m         np\u001b[38;5;241m.\u001b[39mint32,\n\u001b[0;32m    616\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_UINT32_REF:\n\u001b[0;32m    617\u001b[0m         np\u001b[38;5;241m.\u001b[39muint32,\n\u001b[0;32m    618\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_UINT8_REF:\n\u001b[0;32m    619\u001b[0m         np\u001b[38;5;241m.\u001b[39muint8,\n\u001b[0;32m    620\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_UINT16_REF:\n\u001b[0;32m    621\u001b[0m         np\u001b[38;5;241m.\u001b[39muint16,\n\u001b[0;32m    622\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_INT16_REF:\n\u001b[0;32m    623\u001b[0m         np\u001b[38;5;241m.\u001b[39mint16,\n\u001b[0;32m    624\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_INT8_REF:\n\u001b[0;32m    625\u001b[0m         np\u001b[38;5;241m.\u001b[39mint8,\n\u001b[0;32m    626\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_STRING_REF:\n\u001b[0;32m    627\u001b[0m         np\u001b[38;5;241m.\u001b[39mobject,\n\u001b[0;32m    628\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_COMPLEX64_REF:\n\u001b[0;32m    629\u001b[0m         np\u001b[38;5;241m.\u001b[39mcomplex64,\n\u001b[0;32m    630\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_COMPLEX128_REF:\n\u001b[0;32m    631\u001b[0m         np\u001b[38;5;241m.\u001b[39mcomplex128,\n\u001b[0;32m    632\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_INT64_REF:\n\u001b[0;32m    633\u001b[0m         np\u001b[38;5;241m.\u001b[39mint64,\n\u001b[0;32m    634\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_UINT64_REF:\n\u001b[0;32m    635\u001b[0m         np\u001b[38;5;241m.\u001b[39muint64,\n\u001b[0;32m    636\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_BOOL_REF:\n\u001b[0;32m    637\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool,\n\u001b[0;32m    638\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QINT8_REF:\n\u001b[0;32m    639\u001b[0m         _np_qint8,\n\u001b[0;32m    640\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QUINT8_REF:\n\u001b[0;32m    641\u001b[0m         _np_quint8,\n\u001b[0;32m    642\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QINT16_REF:\n\u001b[0;32m    643\u001b[0m         _np_qint16,\n\u001b[0;32m    644\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QUINT16_REF:\n\u001b[0;32m    645\u001b[0m         _np_quint16,\n\u001b[0;32m    646\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QINT32_REF:\n\u001b[0;32m    647\u001b[0m         _np_qint32,\n\u001b[0;32m    648\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_BFLOAT16_REF:\n\u001b[0;32m    649\u001b[0m         _np_bfloat16,\n\u001b[0;32m    650\u001b[0m }\n\u001b[0;32m    652\u001b[0m _QUANTIZED_DTYPES_NO_REF \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m([qint8, quint8, qint16, quint16, qint32])\n\u001b[0;32m    653\u001b[0m _QUANTIZED_DTYPES_REF \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m(\n\u001b[0;32m    654\u001b[0m     [qint8_ref, quint8_ref, qint16_ref, quint16_ref, qint32_ref])\n",
      "File \u001b[1;32mc:\\Downloaded_Apps\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\numpy\\__init__.py:324\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    319\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[1;32m--> 324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtesting\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'object'.\n`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Flatten, Dense, MaxPooling2D, Input, Dropout, Lambda, Rescaling, BatchNormalization\n",
    "from keras.layers import RandomFlip, RandomCrop, RandomZoom, RandomRotation, RandomTranslation, RandomBrightness\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from google.colab import files\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from keras.utils import image_dataset_from_directory\n",
    "from tensorflow.data import AUTOTUNE\n",
    "from tensorflow import random\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.densenet import DenseNet201\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy classifier\n",
    "Let's first observe how a random classifier would work on the input data if it were to only choose a class randomly. This will be considered a baseline for the performance of our networs. (in the way that if a network performs worse than the random classifier, it is a terrible model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dummy_train, x_dummy_test, y_dummy_train, y_dummy_test = train_test_split(x_all, y_all, test_size=0.2, stratify=y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Classifier Accuracy: 0.16614906832298137\n"
     ]
    }
   ],
   "source": [
    "random_classifier = DummyClassifier(strategy='uniform')\n",
    "random_classifier.fit(x_dummy_train, y_dummy_train)\n",
    "y_pred_random = random_classifier.predict(x_dummy_test)\n",
    "\n",
    "accuracy_random = accuracy_score(y_dummy_test, y_pred_random)\n",
    "\n",
    "print(\"Random Classifier Accuracy:\", accuracy_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drive paths\n",
    "PLOT_PATH = \"./drive/MyDrive/Colab_Data/plotsFinal\"\n",
    "INPUT_224_PATH = \"./drive/MyDrive/Colab_Data/224\"\n",
    "INPUT_64_PATH = \"./drive/MyDrive/Colab_Data/64\"\n",
    "MODELS_PATH = \"./drive/MyDrive/Colab_Data/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def plot_accuracy_loss_subplt(history, title, path, fig_size):\n",
    "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=fig_size)\n",
    "  ax1.plot(history.history['accuracy'], label='Model Accuracy')\n",
    "  ax1.plot(history.history['val_accuracy'])\n",
    "  ax1.legend(['train', 'test'])\n",
    "  #ax1.set_xlabel('epoch')\n",
    "  ax1.set_ylabel('accuracy')\n",
    "  ax2.plot(history.history['loss'], label='Model Loss')\n",
    "  ax2.plot(history.history['val_loss'])\n",
    "  ax2.legend(['train', 'test'])\n",
    "  #ax2.set_xlabel('epoch')\n",
    "  ax2.set_ylabel('loss')\n",
    "  plt.subplots_adjust(hspace=0.9)\n",
    "  plt.title(title)\n",
    "  plt.savefig(path)\n",
    "  files.download(path)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data loading in drive\n",
    "test_ds_224 = image_dataset_from_directory(f\"{INPUT_224_PATH}/test\",\n",
    "                                        labels=\"inferred\",\n",
    "                                        label_mode=\"categorical\",\n",
    "                                        color_mode='rgb',\n",
    "                                        batch_size=32,\n",
    "                                        image_size=(224,224),\n",
    "                                        seed=42)\n",
    "test_ds_64 = image_dataset_from_directory(f\"{INPUT_224_PATH}/test\",\n",
    "                                        labels=\"inferred\",\n",
    "                                        label_mode=\"categorical\",\n",
    "                                        color_mode='rgb',\n",
    "                                        batch_size=32,\n",
    "                                        image_size=(64,64),\n",
    "                                        seed=42)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_224 = image_dataset_from_directory(f\"{INPUT_224_PATH}/train\",\n",
    "                                        labels=\"inferred\",\n",
    "                                        label_mode=\"categorical\",\n",
    "                                        color_mode='rgb',\n",
    "                                        batch_size=32,\n",
    "                                        image_size=(224,224),\n",
    "                                        shuffle=True,\n",
    "                                        validation_split=0.111111,\n",
    "                                        subset=\"training\",\n",
    "                                        seed=42)\n",
    "\n",
    "val_ds_224 = image_dataset_from_directory(f\"{INPUT_224_PATH}/train\",\n",
    "                                        labels=\"inferred\",\n",
    "                                        label_mode=\"categorical\",\n",
    "                                        color_mode='rgb',\n",
    "                                        batch_size=32,\n",
    "                                        image_size=(224,224),\n",
    "                                        shuffle=True,\n",
    "                                        validation_split=0.111111,\n",
    "                                        subset=\"validation\",\n",
    "                                        seed=42)\n",
    "\n",
    "train_ds_224 = train_ds_224.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds_224 = val_ds_224.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_64 = image_dataset_from_directory(f\"{INPUT_64_PATH}/train\",\n",
    "                                        labels=\"inferred\",\n",
    "                                        label_mode=\"categorical\",\n",
    "                                        color_mode='rgb',\n",
    "                                        batch_size=32,\n",
    "                                        image_size=(224,224),\n",
    "                                        shuffle=True,\n",
    "                                        validation_split=0.111111,\n",
    "                                        subset=\"training\",\n",
    "                                        seed=42)\n",
    "\n",
    "val_ds_64 = image_dataset_from_directory(f\"{INPUT_64_PATH}/train\",\n",
    "                                        labels=\"inferred\",\n",
    "                                        label_mode=\"categorical\",\n",
    "                                        color_mode='rgb',\n",
    "                                        batch_size=32,\n",
    "                                        image_size=(224,224),\n",
    "                                        shuffle=True,\n",
    "                                        validation_split=0.111111,\n",
    "                                        subset=\"validation\",\n",
    "                                        seed=42)\n",
    "\n",
    "train_ds_64 = train_ds_64.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds_64 = val_ds_64.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common for augmentation\n",
    "augmentation_layers = Sequential(name=\"Augmentation\")\n",
    "augmentation_layers.add(RandomFlip(\"horizontal\"))\n",
    "augmentation_layers.add(RandomTranslation((-0.1, 0.1), (-0.2, 0.2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Simple Architecture\n",
    "Let's first train the model on a network comprised of 3 convolutional layers, 3 pooling layers and a dense neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 64 x 64 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_simple_model_64 = Sequential(name=\"Simple_CNN_64\")\n",
    "cnn_simple_model_64.add(Input(shape=(64, 64, 3)))\n",
    "cnn_simple_model_64.add(Rescaling(1./255))\n",
    "\n",
    "# CNN\n",
    "cnn_simple_model_64.add(Conv2D(filters=32, kernel_size=3, strides=(1, 1), padding=\"same\", activation=\"relu\"))\n",
    "cnn_simple_model_64.add(MaxPooling2D(2))\n",
    "\n",
    "cnn_simple_model_64.add(Conv2D(filters=32, kernel_size=3, strides=(1, 1), padding=\"same\", activation=\"relu\"))\n",
    "cnn_simple_model_64.add(MaxPooling2D(2))\n",
    "\n",
    "# DNN\n",
    "cnn_simple_model_64.add(Flatten())\n",
    "cnn_simple_model_64.add(Dense(units=128, activation=\"relu\"))\n",
    "cnn_simple_model_64.add(Dense(units=64, activation=\"relu\"))\n",
    "cnn_simple_model_64.add(Dense(units=6, activation=\"softmax\"))\n",
    "cnn_simple_model_64.compile(optimizer=RMSprop(learning_rate=0.001), loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "cnn_simple_model_64.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cnn_simple_model_64.fit(x=train_ds_64, validation_data = val_ds_64, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_loss_subplt(history, \"64x64 CNN Simple\", \"./drive/MyDrive/An_IV/PRS/plotsFinal/cnn_simple_64.jpg\", (10, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_simple_model_reg_64 = Sequential(name=\"Simple_CNN_64_Normalized\")\n",
    "cnn_simple_model_reg_64.add(Input(shape=(224, 224, 3)))\n",
    "cnn_simple_model_reg_64.add(Rescaling(1./255))\n",
    "cnn_simple_model_reg_64.add(augmentation_layers)\n",
    "\n",
    "# CNN\n",
    "cnn_simple_model_reg_64.add(Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "cnn_simple_model_reg_64.add(MaxPooling2D(2))\n",
    "\n",
    "cnn_simple_model_reg_64.add(Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "cnn_simple_model_reg_64.add(MaxPooling2D(2))\n",
    "\n",
    "# DNN\n",
    "cnn_simple_model_reg_64.add(Flatten())\n",
    "cnn_simple_model_reg_64.add(Dense(units=128, activation=\"relu\"))\n",
    "cnn_simple_model_reg_64.add(Dropout(0.25))\n",
    "cnn_simple_model_reg_64.add(Dense(units=64, activation=\"relu\"))\n",
    "cnn_simple_model_reg_64.add(Dropout(0.25))\n",
    "cnn_simple_model_reg_64.add(Dense(units=6, activation=\"softmax\"))\n",
    "cnn_simple_model_reg_64.compile(optimizer=RMSprop(learning_rate=0.001), loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "cnn_simple_model_reg_64.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cnn_simple_model_reg_64.fit(x=train_ds_64, validation_data = val_ds_64, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_loss_subplt(history, \"64x64 CNN Simple Regularized\", f\"{PLOT_PATH}/cnn_simple_reg_64.jpg\", (10, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 224 x 224 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple CNN model\n",
    "cnn_simple_224 = Sequential(name=\"Simple_CNN_64\")\n",
    "cnn_simple_224.add(Input(shape=(224, 224, 3)))\n",
    "cnn_simple_224.add(Rescaling(1./255))\n",
    "\n",
    "# CNN\n",
    "cnn_simple_224.add(Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "cnn_simple_224.add(MaxPooling2D(2))\n",
    "\n",
    "cnn_simple_224.add(Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "cnn_simple_224.add(MaxPooling2D(2))\n",
    "\n",
    "# DNN\n",
    "cnn_simple_224.add(Flatten())\n",
    "cnn_simple_224.add(Dense(units=128, activation=\"relu\"))\n",
    "cnn_simple_224.add(Dense(units=64, activation=\"relu\"))\n",
    "cnn_simple_224.add(Dense(units=6, activation=\"softmax\"))\n",
    "cnn_simple_224.compile(optimizer=RMSprop(learning_rate=0.001), loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "cnn_simple_224.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cnn_simple_224.fit(train_ds_224, validation_data=val_ds_224, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_loss_subplt(history, \"224x224 CNN Simple\", f\"{PLOT_PATH}/cnn_simple_224.jpg\", (10, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=./data/plotsFinal/cnn_simple_224.jpg>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_simple_224.save(f\"{MODELS_PATH}/cnn_simple_224.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple CNN model\n",
    "cnn_simple_norm_224 = Sequential(name=\"Simple_CNN_224_Regularized\")\n",
    "cnn_simple_norm_224.add(Input(shape=(224, 224, 3)))\n",
    "cnn_simple_norm_224.add(Rescaling(1./255))\n",
    "cnn_simple_norm_224.add(augmentation_layers)\n",
    "\n",
    "# CNN\n",
    "cnn_simple_norm_224.add(Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "cnn_simple_norm_224.add(MaxPooling2D(2))\n",
    "\n",
    "cnn_simple_norm_224.add(Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "cnn_simple_norm_224.add(MaxPooling2D(2))\n",
    "\n",
    "# DNN\n",
    "cnn_simple_norm_224.add(Flatten())\n",
    "cnn_simple_norm_224.add(Dense(units=128, activation=\"relu\"))\n",
    "cnn_simple_norm_224.add(Dropout(0.25))\n",
    "cnn_simple_norm_224.add(Dense(units=64, activation=\"relu\"))\n",
    "cnn_simple_norm_224.add(Dropout(0.25))\n",
    "cnn_simple_norm_224.add(Dense(units=6, activation=\"softmax\"))\n",
    "cnn_simple_norm_224.compile(optimizer=RMSprop(learning_rate=0.001), loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "cnn_simple_norm_224.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "my_callbacks = [early_stop]\n",
    "\n",
    "history = cnn_simple_norm_224.fit(train_ds_224, batch_size=32, epochs=30, validation_data=val_ds_224, callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=./data/plotsFinal/cnn_s_reg_224.jpg>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_loss_subplt(history, \"224x224 CNN aug norm\", f\"{PLOT_PATH}/cnn_s_reg_224.jpg\", (10, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_simple_norm_224.save(f\"{MODELS_PATH}/cnn_simple_reg_224.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Deeper Architecture\n",
    "We add a few additional layers hoping it will help the model differentiate better between classes.\n",
    "#### 224 x 224 Images\n",
    "We will test the architecture on this dataset as the larger size allows multiple convolutional layers (as size is not as dramatically decreased)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_deeper_224 = Sequential(name=\"Deeper_CNN_64_Normalized\")\n",
    "cnn_deeper_224.add(Input(shape=(224, 224, 3)))\n",
    "cnn_deeper_224.add(Rescaling(1./255))\n",
    "\n",
    "# CNN\n",
    "cnn_deeper_224.add(Conv2D(filters=64, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "cnn_deeper_224.add(Conv2D(filters=64, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "cnn_deeper_224.add(MaxPooling2D(2))\n",
    "\n",
    "cnn_deeper_224.add(Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "cnn_deeper_224.add(Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "cnn_deeper_224.add(MaxPooling2D(2))\n",
    "\n",
    "cnn_deeper_224.add(Conv2D(filters=16, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "cnn_deeper_224.add(MaxPooling2D(2))\n",
    "\n",
    "# DNN\n",
    "cnn_deeper_224.add(Flatten())\n",
    "cnn_deeper_224.add(Dense(units=128, activation=\"relu\"))\n",
    "cnn_deeper_224.add(Dense(units=64, activation=\"relu\"))\n",
    "cnn_deeper_224.add(Dense(units=6, activation=\"softmax\"))\n",
    "cnn_deeper_224.compile(optimizer=RMSprop(learning_rate=0.001), loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "cnn_deeper_224.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "my_callbacks = [early_stop]\n",
    "\n",
    "history = cnn_deeper_224.fit(train_ds_224, batch_size=32, epochs=30, validation_data=val_ds_224, callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_loss_subplt(history, \"224x224 CNN Deep\", f\"{PLOT_PATH}/cnn_deep_224.jpg\", (10, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=./data/plotsFinal/cnn_deep_224.jpg>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_deeper_224.save(f\"{MODELS_PATH}/cnn_deeper_224.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_deeper_reg_224 = Sequential(name=\"Deeper_CNN_64_Normalized\")\n",
    "cnn_deeper_reg_224.add(Input(shape=(224, 224, 3)))\n",
    "cnn_deeper_reg_224.add(Rescaling(1./255))\n",
    "cnn_deeper_reg_224.add(augmentation_layers)\n",
    "\n",
    "# CNN\n",
    "cnn_deeper_reg_224.add(Conv2D(filters=64, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "cnn_deeper_reg_224.add(Conv2D(filters=64, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "cnn_deeper_reg_224.add(MaxPooling2D(2))\n",
    "\n",
    "cnn_deeper_reg_224.add(Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "cnn_deeper_reg_224.add(Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "cnn_deeper_reg_224.add(MaxPooling2D(2))\n",
    "\n",
    "cnn_deeper_reg_224.add(Conv2D(filters=16, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "cnn_deeper_reg_224.add(MaxPooling2D(2))\n",
    "\n",
    "# DNN\n",
    "cnn_deeper_reg_224.add(Flatten())\n",
    "cnn_deeper_reg_224.add(Dense(units=128, activation=\"relu\"))\n",
    "cnn_deeper_reg_224.add(Dropout(0.25))\n",
    "cnn_deeper_reg_224.add(Dense(units=64, activation=\"relu\"))\n",
    "cnn_deeper_reg_224.add(Dropout(0.25))\n",
    "cnn_deeper_reg_224.add(Dense(units=6, activation=\"softmax\"))\n",
    "cnn_deeper_reg_224.compile(optimizer=RMSprop(learning_rate=0.001), loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "cnn_deeper_reg_224.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "my_callbacks = [early_stop]\n",
    "\n",
    "history = cnn_deeper_reg_224.fit(train_ds_224, batch_size=32, epochs=30, validation_data=val_ds_224, callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_loss_subplt(history, \"224x224 CNN Deep Reg\", f\"{PLOT_PATH}/cnn_deep_reg_224.jpg\", (10, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=./data/plotsFinal/cnn_deep_reg_224.jpg>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_deeper_reg_224.save(f\"{MODELS_PATH}/cnn_deeper_reg_224.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning VGG16\n",
    "Transfer learning refers to taking advantage of previously trained models that proved successfult for a similar task and fine tuning them using the data.\n",
    "For this we will only use 224x224 data as it is the standard for models such as VGG16, ResNet, DenseNet201 etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(224,224,3))\n",
    "rescaling = Rescaling(1./255)(input_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_vgg = VGG16(weights=\"imagenet\", include_top=False, input_tensor=rescaling)\n",
    "trained_vgg.trainable = False\n",
    "trained_vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include the dense neural network at the end\n",
    "complete_model_vgg = Sequential()\n",
    "complete_model_vgg.add(trained_vgg)\n",
    "complete_model_vgg.add(Flatten())\n",
    "complete_model_vgg.add(Dense(units=40, activation=\"relu\"))\n",
    "complete_model_vgg.add(Dense(units=20, activation=\"relu\"))\n",
    "complete_model_vgg.add(Dense(units=6, activation=\"softmax\"))\n",
    "\n",
    "complete_model_vgg.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "complete_model_vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = complete_model_vgg.fit(x=train_ds_224, validation_data=val_ds_224, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_loss_subplt(history, \"224x224 VGG\", f\"{PLOT_PATH}/vgg_tf_224.jpg\", (10, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=./data/plotsFinal/vgg_tf_224.jpg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_model__vgg_reg = Sequential()\n",
    "complete_model__vgg_reg.add(augmentation_layers)\n",
    "complete_model__vgg_reg.add(trained_vgg)\n",
    "complete_model__vgg_reg.add(Flatten())\n",
    "complete_model__vgg_reg.add(Dense(units=40, activation=\"relu\"))\n",
    "complete_model__vgg_reg.add(Dropout(0.05))\n",
    "complete_model__vgg_reg.add(Dense(units=20, activation=\"relu\"))\n",
    "complete_model__vgg_reg.add(Dense(units=6, activation=\"softmax\"))\n",
    "\n",
    "complete_model__vgg_reg.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "my_callbacks = [early_stop]\n",
    "\n",
    "history = complete_model__vgg_reg.fit(x=train_ds_224, validation_data=val_ds_224, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_loss_subplt(history, \"224x224 VGG\", f\"{PLOT_PATH}/vgg_tf_reg_224.jpg\", (10, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=./data/plotsFinal/vgg_tf_reg_224.jpg>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_model__vgg_reg.save(f\"{MODELS_PATH}/vgg_tf_reg_224.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning DenseNet201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_densenet = DenseNet201(weights=\"imagenet\", include_top=False, input_tensor=rescaling)\n",
    "trained_densenet.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_model_densenet = Sequential()\n",
    "complete_model_densenet.add(trained_densenet)\n",
    "complete_model_densenet.add(Flatten())\n",
    "complete_model_densenet.add(Dense(units=40, activation=\"relu\"))\n",
    "complete_model_densenet.add(Dense(units=20, activation=\"relu\"))\n",
    "complete_model_densenet.add(Dense(units=6, activation=\"softmax\"))\n",
    "\n",
    "complete_model_densenet.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "complete_model_densenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "my_callbacks = [early_stop]\n",
    "\n",
    "history = complete_model_densenet.fit(x=train_ds_224, validation_data=val_ds_224, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_loss_subplt(history, \"224x224 DN\", f\"{PLOT_PATH}/dn_tf_224.jpg\", (10, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_model_densenet.save(f\"{MODELS_PATH}/dn_tf_224.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "For evaluating each network we run the evaluate method on the test set (which was partitioned at the beginning, and was not used at all until now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_224_model = load_model(f\"{MODELS_PATH}/cnn_simple_224.keras\")\n",
    "test_loss, test_accuracy = simple_224_model.evaluate(test_ds_224, verbose=2)\n",
    "print('\\nSimple Test accuracy: ', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_reg_224_model = load_model(f\"{MODELS_PATH}/cnn_simple_reg_224.keras\")\n",
    "test_loss, test_accuracy = simple_reg_224_model.evaluate(test_ds_224, verbose=2)\n",
    "print('\\nSimple Reg Test accuracy: ', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeper_244_model = load_model(f\"{MODELS_PATH}/cnn_deeper_224.keras\")\n",
    "test_loss, test_accuracy = deeper_244_model.evaluate(test_ds_224, verbose=2)\n",
    "print('\\nDeeper Test accuracy: ', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeper_reg_244_model = load_model(f\"{MODELS_PATH}/cnn_deeper_reg_224.keras\")\n",
    "test_loss, test_accuracy = deeper_reg_244_model.evaluate(test_ds_224, verbose=2)\n",
    "print('\\nDeeper Test Reg accuracy: ', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_reg_model = load_model(f\"{MODELS_PATH}/vgg_tf_reg_224.keras\")\n",
    "test_loss, test_accuracy = vgg_reg_model.evaluate(test_ds_224, verbose=2)\n",
    "print('\\nDeeper Test accuracy: ', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnet_model = load_model(f\"{MODELS_PATH}/dn_tf_224.keras\")\n",
    "test_loss, test_accuracy = dnet_model.evaluate(test_ds_224, verbose=2)\n",
    "print('\\nDeeper Test accuracy: ', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Network Architecture|Hidden test set accuracy|\n",
    "|-|-|\n",
    "|Simple CNN 224|26.39%|\n",
    "|Simple CNN Regularized|35.55%|\n",
    "|Deeper CNN|34.36%|\n",
    "|Deeper CNN Regularized|34.78%|\n",
    "|VGG Regularized|39.28%|\n",
    "|DenseNet|TBD - 30% approx on validation during training|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom images evaluation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv.imread(f'./data/1.JPG')\n",
    "image_rgb = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "image_rgb_resized = cv.resize(image_rgb, (224,224), cv.INTER_NEAREST)\n",
    "input_tensor = np.expand_dims(image_rgb_resized, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vgg_reg_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mvgg_reg_model\u001b[49m(input_tensor)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vgg_reg_model' is not defined"
     ]
    }
   ],
   "source": [
    "vgg_reg_model(input_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_si",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
